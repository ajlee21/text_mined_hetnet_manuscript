## Introduction

Knowledge bases are important resources that hold complex structured and unstructed information. 
These resources have been used in important tasks such as network analysis for drug repurposing discovery [@doi:10.1371/journal.pone.0084912; @doi:10.1101/385617; @doi:10.7554/eLife.26726] or as a source of training labels for text mining systems [@doi:10.3115/1690219.1690287; @doi:10.1101/444398; @doi:10.1186/s12859-019-2873-7]. 
Populating knowledge bases often requires highly-trained scientists to read biomedical literature and summarize the results [@doi:10.1093/bib/bbn043].
This manual curation process requires a significant amount of effort and time: in 2007 researchers estimated that filling in the missing annotations would require approximately 8.4 years [@doi:10.1093/bioinformatics/btm229].
The rate of publications has continued to increase exponentially [@doi:10.1002/asi.23329].
This has been recognized as a considerable challenge, which can lead to gaps in knowledge bases [@doi:10.1093/bioinformatics/btm229].  
Relationship extraction has been studied as a solution towards handling this problem [@doi:10.1093/bib/bbn043].
This process consists of creating a machine learning system to automatically scan and extract relationships from textual sources.
Machine learning methods often leverage a large corpus of well-labeled training data, which still requires manual curation.
Distant supervision is one technique to sidestep the requirement of well-annotated sentences: with distant supervision one makes the assumption that all sentences containing an entity pair found in a selected database provide evidence for a relationship [@doi:10.3115/1690219.1690287].
Distant supervision provides many labeled examples; however it is accompanied by a decrease in the quality of the labels.  
Ratner et al. [@arxiv:1605.07723] recently introduced "data programming" as a solution.
Data programming combines distant supervision with the automated labeling of text using hand-written label functions.
The distant supervision sources and label functions are integrated using a noise aware generative model that is used to produce training labels.
Combining distant supervision with label functions can dramatically reduce the time required to acquire sufficient training data.
However, constructing a knowledge base of heterogeneous relationships through this framework still requires tens of hand-written label functions for each relationship type.
Writing useful label functions requires significant error analysis, which can be a time-consuming process.  

In this paper, we aim to address the question: to what extent can label functions be re-used across different relationship types?
We hypothesized that sentences describing one relationship type may share information in the form of keywords or sentence structure with sentences that indicate other relationship types.
We designed a series of experiments to determine the extent to which label function re-use enhanced performance over distant supervision alone.
We examined relationships that indicate similar types of physical interactions (i.e., gene-binds-gene and compound-binds-gene) as well as different types (i.e., disease-associates-gene and compound-treats-disease).
The re-use of label functions could dramatically reduce the number required to generate and update a heterogeneous knowledge graph.

### Related Work

Relationship extraction is the process of detecting and classifying semantic relationships from a collection of text.
This process can be broken down into three different categories: (1) the use of natural language processing techniques such as manually crafted rules and the identification of key text patterns for relationship extraction, (2) the use of unsupervised methods via co-occurrence scores or clustering, and (3) supervised or semi-supervised machine learning using annotated datasets for the classification of documents or sentences.
In this section, we discuss selected efforts for each type of edge that we include in this project.

#### Disease-Gene Associations 

Efforts to extract Disease-associates-Gene (DaG) relationships have often used manually crafted rules or unsupervised methods.
One study used hand crafted rules based on a sentence's grammatical structure, represented as dependency trees, to extract DaG relationships [@tag:pkde4j].
Some of these rules inspired certain DaG text pattern label functions in our work.
Another study used co-occurrence frequencies within abstracts and sentences to score the likelihood of association between disease and gene pairs [@tag:diseases].
The results of this study were incorporated into Hetionet v1 [@doi:10.7554/eLife.26726], so this served as one of our distant supervision label functions.
Another approach built off of the above work by incorporating a supervised classifier, trained via distant supervision, into a scoring scheme [@tag:cocoscore].
Each sentence containing a disease and gene mention is scored using a logistic regression model and combined using the same co-occurrence approach used in Pletscher-Frankild et al. [@tag:diseases].
We compared our results to this approach to measure how well our overall method performs relative to other methods.
Besides the mentioned three studies, researchers have used co-occurrences for extraction alone [@tag:lgscore; @tag:polysearch; @tag:full_text_co_abstracts] or in combination with other features to recover DaG relationships [@tag:dg_text_pubmed].
One recent effort relied on a bi-clustering approach to detect DaG-relevant sentences from Pubmed abstracts [@tag:global_network] with clustering of dependency paths grouping similar sentences together.
The results of this work supply our domain heuristic label functions.
These approaches do not rely on a well-annotated training performance and tend to provide excellent recall, though the precision is often worse than with supervised methods [@doi:10.1038/nrg1768; @doi:10.1016/j.ymeth.2015.01.015].

Hand-crafted high-quality datasets [@tag:befree; @tag:eu_adr; @tag:comagc; @tag:craft] often serve as a gold standard for training, tuning, and testing supervised machine learning methods in this setting.
Support vector machines have been repeatedly used to detect DaG relationships [@tag:befree; @tag:dtminer; @tag:ensemble_svm].
These models perform well in large feature spaces, but are slow to train as the number of data points becomes large.
Recently, some studies have used deep neural network models.
One used a pre-trained recurrent neural network [@tag:biobert], and another used distant supervision [@tag:deep_dive_dag].
Due to the success of these two models, we decided to a deep neural network as our discriminative model.

#### Compound Treats Disease

The goal of extracting Compound-treats-Disease (CtD) edges is to identify sentences that mention current drug treatments or propose new uses for existing drugs.
One study combined an inference model from previously established drug-gene and gene-disease relationships to infer novel drug-disease interactions via co-occurrences [@tag:abc_drugs].
A similar approach has also been applied to CtD extraction [@tag:copub_discovery].
Manually-curated rules have also been applied to PubMed abstracts to address this task [@tag:ctd_medline].
The rules were based on identifying key phrases and wordings related to using drugs to treat a disease, and we used these patterns as inspirations for some of our CtD label functions. 
Lastly, one study used a  bi-clustering approach to identify sentences relevant to CtD edges [@tag:global_network].
As with DaG edges, we use the results from this study to provide what we term as domain heuristic label functions.

Recent work with supervised machine learning methods has often focused on compounds that induce a disease: an important question for toxicology and the subject of the BioCreative V dataset [@tag:biocreative_v].
We don't consider environmental toxicants in our work, as our source databases for distant supervision are primarily centered around FDA-approved therapies.

#### Compound Binds Gene

The BioCreative VI track 5 task focused on classifying compound-protein interactions and has led to a great deal of work on the topic [@raw:biocreative/chemprot].
The equivalent edge in our networks is Compound-binds-Gene (CbG).
Curators manually annotated 2,432 PubMed abstracts for five different compound protein interactions (agonist, antagonist, inhibitor, activator and substrate/product production) as part of the BioCreative task. 
The best performers on this task achieved an F1 score of 64.10% [@raw:biocreative/chemprot].
Numerous additional groups have now used the publicly available dataset, that resulted from this competition, to train supervised machine learning methods [@tag:limtox; @tag:lptk; @tag:cbg_ensemble_dl; @tag:biobert; @tag:cbg_ensemble_dl; @tag:cbg_feat_engineering; @tag:cbg_transfer_learning; @tag:cbg_neural_attention; @tag:recursive_nn] and semi-supervised machine learning methods [@tag:semi_supervised_vae].
These approaches depend on well-annotated training datasets, which creates a bottleneck.
In addition to supervised and semi-supervised machine learning methods, hand crafted rules [@tag:pharmpresso] and bi-clustering of dependency trees  [@tag:global_network] have been used.
We use the results from the bi-clustering study to provide a subset of the CbG label functions in this work.

#### Gene-Gene Interactions

Akin to the DaG edge type, many efforts to extract Gene-interacts-Gene (GiG) relationships used co-occurrence approaches.
This edge type is more frequently referred to as a protein-protein interaction.
Even approaches as simple as calculating Z-scores from PubMed abstract co-occurrences can be informative [@tag:string], and there are numerous studies using co-occurrences [@tag:ppinterfinder; @tag:hpiminer; @tag:protein_protein_co_network; @tag:full_text_co_abstracts].
However, more sophisticated strategies such as distant supervision appear to improve performance [@tag:cocoscore].
Similarly to the other edge types, the bi-clustering approach over dependency trees has also been applied to this edge type [@tag:global_network].
This manuscript provides a set of label functions for our work.
These methods benefit from not needing annotated data and tend to have good recall performance.

Most supervised classifiers used publicly available datasets for evaluation [@tag:aimed; @tag:bioinfer; @raw:LLL; @tag:hprd50; @raw:IEPA].
These datasets are used equally among studies, but can generate noticable differences in terms of performance [@doi:10.1186/1471-2105-9-S3-S6].
Support vector machines were a common approach to extract GiG edges [@tag:ppi_graph_kernels; @tag:protein_docking].
However, with the growing popularity of deep learning numerous deep neural network architectures have been applied [@tag:ppi_bilstm; @tag:ppi_deep_conv; @tag:mcdepcnn; @tag:semi_supervised_vae].
Distant supervision has also been used in this domain [@tag:deep_dive], and in fact this effort was one of the motivating rationales for our work.
