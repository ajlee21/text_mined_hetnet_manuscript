## Introduction

Knowledge bases are important resources that hold complex structured and unstructed information. 
These resources have been used in important tasks such as network analysis for drug repurposing discovery [@doi:10.1371/journal.pone.0084912; @doi:10.1101/385617; @doi:10.7554/eLife.26726] or as a source of training labels for text mining systems [@doi:10.3115/1690219.1690287; @doi:10.1101/444398; @doi:10.1186/s12859-019-2873-7]. 
Populating knowledge bases often requires highly-trained scientists to read biomedical literature and summarize the results [@doi:10.1093/bib/bbn043].
This manual curation process requires a significant amount of effort and time: in 2007 researchers estimated that filling in the missing annotations at that point would require approximately 8.4 years [@doi:10.1093/bioinformatics/btm229]).
The rate of publications has continued to increase exponentially [@doi:10.1002/asi.23329].
This has been recognized as a considerable challenge and leads to gaps in knowledge bases [@doi:10.1093/bioinformatics/btm229].  
Relationship extraction has been studied as a solution towards handling this problem [@doi:10.1093/bib/bbn043].
This process consists of creating a machine learning system to automatically scan and extract relationships from textual sources.
Machine learning methods often leverage a large corpus of well-labeled training data, which still requires manual curation.
Distant supervision is one technique to sidestep the requirement of well-annotated sentences: with distant supervision one makes the assumption that that all sentences containing an entity pair found in a selected database provide evidence for a relationship [@doi:10.3115/1690219.1690287].
Distant supervision provides many labeled examples; however it is accompanied by a decrease in the quality of the labels.  
Ratner et al. [@arxiv:1605.07723] recently introduced "data programming" as a solution.
Data programming combines distant supervision with the automated labeling of text using hand-written label functions.
The distant supervision sources and label functions are integrated using a noise aware generative model, which is used to produce training labels.
Combining distant supervision with label functions can dramatically reduce the time required to acquire sufficient training data.
However, constructing a knowledge base of heterogeneous relationships through this framework still requires tens of hand-written label functions for each relationship type.
Writing useful label functions requires significant error analysis, which can be a time-consuming process.  

In this paper, we aim to address the question: to what extent can label functions be re-used across different relationship types?
We hypothesized that sentences describing one relationship type may share information in the form of keywords or sentence structure with sentences that indicate other relationship types.
We designed a series of experiments to determine the extent to which label function re-use enhanced performance over distant supervision alone.
We examine relationships that indicate similar types of physical interactions (i.e., gene-binds-gene and compound-binds-gene) as well as different types (i.e., disease-associates-gene and compound-treats-disease).
The re-use of label functions could dramatically reduce the number required to generate and update a heterogeneous knowledge graph.

### Related Work

Relationship extraction is the process of detecting and classifying semantic relationships from a collection of text.
This process can be broken down into three different categories: (1) the use of natural language processing techniques such as manually crafted rules and identifying key text patterns for relationship extraction, (2) the use of unsupervised methods via co-occurrence scores or clustering, and (3) supervised or semi-supervised machine learning using annotated datasets for classification of documents or sentences.
In this section, we discuss selected efforts for each type of edge that we included in this project.

#### Disease-Gene Associations 

Substantial effort for Disease-associates-Gene (DaG) extraction comprised of using manually crafted rules and unsupervised methods.
One study used hand crafted rules on generated dependency trees to extract Disease-associates-Gene relationships [@tag:pkde4j].
These rules were based off of a sentence's grammatical structure and labeled any structure match as positive.
We used some of these rules as inspiration for our DaG text pattern label functions.
Another study used a co-occurrence approach to score the likelihood of association between disease and gene pairs [@tag:diseases].
These scores were calculated using the frequency of disease and gene mentions within abstracts and within sentences.
The results of this study was incorporated into Hetionet v1 and used as one of our distant supervision label functions.
Another approach built off of the previously mentioned approach by using a distant supervision technique with a logistic regression model to improve the frequency calculations [@tag:cocoscore]. 
We incorporate this work as a baseline comparison for our discriminative model (Results Section: Reconstructing Hetionet).
Besides the mentioned two studies, other work have used pure co-occurrence for extraction [@tag:lgscore; @tag:polysearch; @tag:full_text_co_abstracts] or a combination of scores with other features to mine the DaG relationship [@tag:dg_text_pubmed ].
Along with co-occurrence, one study used a bi-clustering approach to detect DaG relevant sentences from Pubmed abstracts [@tag:global_network].
This clustering approach used dependency paths to group similar sentences together.
We used the results of this work for our domain heuristic label functions.
The benefit of using the above approaches is inessential need for annotated data.
These methods provide great recall performance in terms of extraction, but can suffer in precision performance.

Supervised machine learning for DaG has prospered with the creation of hand crafted datasets [@tag:befree; @tag:eu_adr; @tag:comagc; @tag:craft].
This datasets served as a gold standard for tuning and testing machine learning models in DaG extraction.
Copious amount of effort used support vector machines to detect DaG relationships [@tag:befree; @tag:dtminer; @tag:ensemble_svm].
This model performs well in large feature spaces, but perform slowly as the number of data points increases.
Following support vector machines, effort has focuses on using deep learning models for extraction.
One such effort used a pre-trained recurrent neural network for DaG extraction [@tag:biobert].
Similar to our approach another study used distant supervision for DaG extraction [@tag:deep_dive_dag].
Due to the success of these two models, we take a deep learning approach for our discriminative model.

#### Compound Treats Disease

Effort for Compound-treats-Disease (CtD) extraction focuses on identifying sentences that mention current drug treatments or discovering new uses for existing drugs (drug repurposing).
One study used an inference model combined with co-occurrence scores to identify sentences regarding drug repurposing predictions [@tag:abc_drugs].
This approach consisted of using an inference model on previously established drug-gene and gene-disease relationships to infer novel drug-disease interactions.
Each identified pair was ranked using a co-occurrence score that was calculated using pair mention frequency on Pubmed abstracts [@tag:abc_drugs].
A similar study used a similar inference model for CtD extraction [@tag:copub_discovery].
Following co-occurrence one study consisted of using manually curated rules on Pubmed abstracts for CtD extraction [@tag:ctd_medline].
These rules were based on identifying key drug treatment phrases and wording for extraction. 
We used these patterns as inspiration for some of our CtD specified label functions.
Lastly, one study used a  bi-clustering approach to identify sentences relevant to the CtD [@tag:global_network].
As mentioned for DaG we use the results of this study for domain heuristic label functions.

Majority of supervised machine learning efforts use the Biocreative V  dataset [@tag:biocreative_v].
This dataset focused on extracting sentences indicating the Compound-induces-Disease relationship.
Incorporating this edge into Hetionet would be an interesting idea, but it is tangential to this work.

#### Compound Binds Gene

Majority of effort for Compound-binds-Gene extraction focuses on the Biocreative task VI track 5 shared task [@raw:biocreative/chemprot].
This shared task challenged contestants to classify compound-protein (genes for our context) interactions.
Curators manually annotated 2,432 PubMed abstracts for five different compound protein interactions (agonist, antagonist, inhibitor, activator and substrate/product production). 
For this task performance was reported in terms of F1 score and the best team achieved a score of 64.10% [@raw:biocreative/chemprot].
This competition provided a publicly available dataset that copious amount of supervised machine learning methods use [@tag:limtox; @tag:lptk; @tag:cbg_ensemble_dl; @tag:biobert; @tag:cbg_ensemble_dl; @tag:cbg_feat_engineering; @tag:cbg_transfer_learning; @tag:cbg_neural_attention; @tag:recursive_nn].
One study used a semi-supervised approach to extract compound-gene interactions [@tag:semi_supervised_vae]. 
Using already annotated datasets the authors used a variational auto encoder consisting of a bidirectional long short term memory network for encoding and decoding and a convolutional neural network for classification.
Their method depends on available annotated datasets, which creates a bottleneck for extracting less studied relationships.
  
Concerning unsupervised methods, one study used hand crafted rules for CbG extraction [@tag:pharmpresso].
Another study used a bi-clustering approach on dependency trees  to identify key sentences for CbG extraction [@tag:global_network].
We used results from the latter study as label functions for CbG extraction.

#### Gene-Gene Interactions

Like Disease-associates-Gene, a plethora of work the for Gene-interacts-Gene (GiG), aka protein-protein interactions, relationship  consisted of co-occurrence approaches.
One study used a Z-score approach to find significant protein pairs from Pubmed abstracts [@tag:string].
A later approach builds off of this mentioned method by using distant supervision and a logistic model to improve performance [@tag:cocoscore].
Following the above methods other, studies used co-occurrence approaches for CbG Extraction [@tag:ppinterfinder; @tag:hpiminer; @protein_protein_co_network; @tag:full_text_co_abstracts].
Similar to the above relationships, one study used a bi-clustering approach on dependency trees for GiG extraction. [tag:global_network].
We use the results of this study for our domain heuristic label functions for GiG extraction.
These methods benefit from not needing annotated data.
Plus, these methods provide great recall performance in terms of extraction.

Majority of supervised machine learning methods used publicly available datasets [@tag:aimed; @tag:bioinfer; @raw:LLL; @tag:hprd50; @raw:IEPA] as a gold standard.  
Significant amount of effort used support vector machines to detect protein-protein interactions [@tag:ppi_graph_kernels; @tag:protein_docking].
As deep learning became popular other studies used such algorithms [@tag:ppi_bilstm; @tag:ppi_deep_conv; @tag:mcdepcnn].
As mentioned in the Compound-binds-Gene section one study used a semi-supervised approach for GiG extraction [@tag:semi_supervised_vae].
This method is relies on the quality of annotated datasets for performance.
This dependence creates a bottleneck for extracting less studied relationships.
Similar to our discriminative model one study used a convolutional neural network to detect protein-protein interactions [@tag:mcdepcnn].
Lastly, one study used a distant supervision approach for GiG extraction [@tag:deep_dive].
Their work uses probabilistic inference and factor graphs to generate predictions. 
Success from this study served as rationale for our work, where we train a generative model to probabilistically annotate sentence then use a discriminative model for classification.
