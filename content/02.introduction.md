## Introduction

Knowledge bases are important resources that hold complex structured and unstructed information. 
These resources have been used in important tasks such as network analysis for drug repurposing discovery [@doi:10.1371/journal.pone.0084912; @doi:10.1101/385617; @doi:10.7554/eLife.26726] or as a source of training labels for text mining systems [@doi:10.3115/1690219.1690287; @doi:10.1101/444398; @doi:10.1186/s12859-019-2873-7]. 
Populating knowledge bases often requires highly-trained scientists to read biomedical literature and summarize the results [@doi:10.1093/bib/bbn043].
This manual curation process requires a significant amount of effort and time: in 2007 researchers estimated that filling in the missing annotations would require approximately 8.4 years [@doi:10.1093/bioinformatics/btm229].
The rate of publications has continued to increase exponentially [@doi:10.1002/asi.23329].
This has been recognized as a considerable challenge, which can lead to gaps in knowledge bases [@doi:10.1093/bioinformatics/btm229].  
Relationship extraction has been studied as a solution towards handling this problem [@doi:10.1093/bib/bbn043].
This process consists of creating a machine learning system to automatically scan and extract relationships from textual sources.
Machine learning methods often leverage a large corpus of well-labeled training data, which still requires manual curation.
Distant supervision is one technique to sidestep the requirement of well-annotated sentences: with distant supervision one makes the assumption that all sentences containing an entity pair found in a selected database provide evidence for a relationship [@doi:10.3115/1690219.1690287].
Distant supervision provides many labeled examples; however it is accompanied by a decrease in the quality of the labels.  
Ratner et al. [@arxiv:1605.07723] recently introduced "data programming" as a solution.
Data programming combines distant supervision with the automated labeling of text using hand-written label functions.
The distant supervision sources and label functions are integrated using a noise aware generative model that is used to produce training labels.
Combining distant supervision with label functions can dramatically reduce the time required to acquire sufficient training data.
However, constructing a knowledge base of heterogeneous relationships through this framework still requires tens of hand-written label functions for each relationship type.
Writing useful label functions requires significant error analysis, which can be a time-consuming process.  

In this paper, we aim to address the question: to what extent can label functions be re-used across different relationship types?
We hypothesized that sentences describing one relationship type may share information in the form of keywords or sentence structure with sentences that indicate other relationship types.
We designed a series of experiments to determine the extent to which label function re-use enhanced performance over distant supervision alone.
We examined relationships that indicated similar types of physical interactions (i.e., gene-binds-gene and compound-binds-gene) as well as different types (i.e., disease-associates-gene and compound-treats-disease).
The re-use of label functions could dramatically reduce the number required to generate and update a heterogeneous knowledge graph.

### Related Work

Relationship extraction is the process of detecting semantic relationships from a collection of text.
This process can be broken down into three different categories: (1) the use of natural language processing techniques such as manually crafted rules and heuristics for relationship extraction, (2) the use of unsupervised methods such as co-occurrence scores or clustering to find patterns within sentences and documents documents, and (3) the use of supervised or semi-supervised machine learning for classifying the presence of a relation within documents or sentences.
In this section, we briefly discuss selected efforts under each category.

#### Rule Based Extractors

Rule based extractors rely heavily on expert knowledge to perform extraction.
Typically, these systems use linguistic rules and heuristics to identify key sentences or phrases.
For example, a hypothetical extractor focused on protein phosphorylation events would identify sentences containing the phrase "gene X phosphorylates gene Y" [@doi:10.1109/TCBB.2014.2372765].
This word is a straightforward indication that two genes have a fundamental role in protein phosphorylation.
Other phrase extractors have been used to identify drug-disease treatments [@tag:ctd_medline], pharmcogenomic events [@tag:pharmpresso] and protein-protein interactions [@tag:ppinterfinder; @tag:hpiminer].
These extractors provide a simple but effective way to extract sentences; however, they depend on extensive knowledge about the text to be properly constructed.

Besides using key phrases, a sentence's grammatical structure can be utilized to preform relationship extraction.
Extractors under this approach involve the use of dependency trees.
These trees are data structures that depict a sentence's grammatical relation structure in the form of nodes and edges.
Nodes represent words and the edges represent the dependency type each word shares between one another.
For example, a possible extractor would classify sentences as a positive if a sentence contained the following dependency tree path: "gene X (subject)-> promotes (verb)<- cell death (direct object) <- in (preposition) <-tumors (object of preposition)" [@tag:pkde4j].
This approach provide extremely precise results, but the quantity of positive results remains modest as sentences appear in distinct forms and structure.
Because of this limitation, recent approaches have incorporated methods on top of rule based extractors such as co-occurrence and machine learning systems [@doi:10.1186/s12859-018-2103-8; @tag:limtox].
We discuss the pros and cons of added methods in a later section.
For this project, we took a blind approach to construct our label functions; however, the discussed approaches in this section provides substantial inspiration for novel label functions in future endeavors.

#### Unsupervised Extractors

Unsupervised extractors detect relationships without the need of annotated text.
Notable approaches exploit the fact that two entities can occur together in text.
This event is referred to as co-occurrence.
Extractors utilize these events in by generating statistics on the frequency of entity pairs occurring in text.
For example, a possible extractor would say gene X is associated with disease Y, because gene X and disease Y appear together more often than individually [@tag:diseases].
This approach has been used to establish the following relationship types: disease-gene relationships [@tag:diseases; @tag:polysearch; @tag:dg_text_pubmed; @tag:lgscore; @tag:full_text_co_abstracts; @tag:copub_discovery], protein-protein interactions [@tag:protein_protein_co_network; @doi:10.1093/database/bau012; @tag:full_text_co_abstracts], drug-disease treatments [@tag:abc_drugs], and tissue-gene relations [@doi:10.7717/peerj.1054].
Extractors using the co-occurrence strategy provide exceptional recall results; however, these methods may fail to detect underreported relationships, because they depend on entity-pair frequency for detection.
Junge et al. created a hybrid approach to account for this issue  [@tag:cocoscore].
Their approach utilized distant supervision to train a classifier to learn the context of each sentence.
Once training had been completed, they used the classifier to score every sentence within their corpus.
Each sentence's score was incorporated into calculating co-occurrence frequencies to establish relationship existence [@tag:cocoscore].
Co-occurrence approaches are powerful in establishing edges on the global scale; however, they cannot identify individual sentences without the need for supervised methods.  

Clustering is an unsupervised approach that extracts relationships from text by group similar sentences together.
A notable extractor used this technique to group sentences based on their grammatical structure [@tag:global_network].
Using Stanford's Core NLP Parser [@doi:10.3115/v1/P14-5010] a dependency tree was generated.
Each tree was clustered based on similarity and each cluster was manually annotated to determine which relationship each group represented [@tag:global_network].
For our project we incorporated the results of this work as domain heuristic label functions.
Overall, unsupervised approaches are desirable since they do not require well-annotated training data. 
These approaches provide excellent recall; however, performance can be limited in terms of precision when compared to supervised machine learning methods [@doi:10.1038/nrg1768; @doi:10.1016/j.ymeth.2015.01.015].

#### Supervised Extractors

Supervised extractors consist of training a machine learning classifier and predict the existence of a relationship.
These classifiers require access to well-annotated datasets, which are usually created via some form of manual curation.
Previous work consists of research experts curating their own datasets to train classifiers [@tag:befree; @tag:eu_adr; @tag:comagc; @tag:craft; @tag:aimed; @tag:bioinfer; @raw:LLL; @tag:hprd50; @raw:IEPA]; however, there have been community-wide efforts to create datasets for shared tasks [@tag:biocreative_v; @raw:biocreative/chemprot; @doi:10.1186/1471-2105-9-S3-S6].
Shared tasks are open challeges that aim to build the best classifier for natural language processing tasks such as named entity tagging or relationship extraction. 
Notable example would be the BioCreative community that hosted a number of shared tasks such as predicting compound-protein interactions (BioCreative VI track 5) [@raw:biocreative/chemprot] and compound induced diseases [@doi:10.1186/1471-2105-9-S3-S6].
Often these datasets are well annotated, but are modest in size (2,432 abstracts [@raw:biocreative/chemprot] for BioCreative VI and 1500 abstracts for BioCreative V [@doi:10.1186/1471-2105-9-S3-S6]).
As machine learning classifiers become increasingly complex, these small dataset sizes cannot suffice.
Plus, these multitude of datasets are uniquely annotated which can generate noticeable differences in terms of classifier performance [@doi:10.1186/1471-2105-9-S3-S6].
Overall, obtaining large well-annotated datasets still remains as an open non-trivial task.

Before the rise of deep learning, a classifier that was most frequently used was support vector machines.
This classifier uses a projection function called a kernel to map data into a high dimensional space so datapoints can be easily discerned between classes [@doi:10.1109/5254.708428].
This method was used to extract disease-gene associations [@tag:befree; @tag:dtminer; @tag:ensemble_svm], protein-protein interactions[@tag:ppi_graph_kernels; @tag:limtox; @tag:lptk] and protein docking information [@tag:protein_docking].
Generally, svms perform well on small datasets with large feature spaces, but are slow to train as the number of datapoints becomes asymptotically large.

Deep learning has been increasingly popular throughout the decades as these methods can outperform common machine learning methods [@doi:10.1016/j.neunet.2014.09.003].
Approaches in this field consist of using various neural network architectures, such as recurrent neural networks [@tag:ppi_bilstm; @tag:cbg_ensemble_dl; @tag:cbg_neural_attention; @tag:recursive_nn; @tag:semi_supervised_vae; @tag:biobert] and convolutional neural networks [@tag:ppi_deep_conv; @tag:mcdepcnn; @tag:cbg_ensemble_dl;@tag:semi_supervised_vae; @tag:cbg_transfer_learning], to extract relationships from text.
In fact approaches in this field were the winning model within the BioCreative VI shared task [@raw:biocreative/chemprot; @raw:chemprot_winner].
Despite the large success of these models, they often require large amounts of data to perform well.
Obtaining these large datasets is a time consuming tasks, which makes training these models a non-trivial task.
Distant supervision has been used as solution to fix the barren amount of large datasets [@doi:10.3115/1690219.1690287].
Approaches have used this paradigm to extract chemical-gene interactions [@tag:semi_supervised_vae], disease-gene associations [@tag:cocoscore] and protein-protein interactions [@tag:deep_dive; @tag:cocoscore; @tag:semi_supervised_vae].
In fact efforts done in [@tag:deep_dive] served as one of the motivating rationales for our work.
Overall, deep learning has provided exceptional results in terms of relationships extraction and we decided to use a deep neural network as our discriminative model.


