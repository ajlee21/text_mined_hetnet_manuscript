## Introduction

Knowledge bases are important resources that hold complex structured and unstructed information. 
These resources have been used in important tasks such as network analysis for drug repurposing discovery [@doi:10.1371/journal.pone.0084912; @doi:10.1101/385617; @doi:10.7554/eLife.26726] or as a source of training labels for text mining systems [@doi:10.3115/1690219.1690287; @doi:10.1101/444398; @doi:10.1186/s12859-019-2873-7]. 
Populating knowledge bases often requires highly-trained scientists to read biomedical literature and summarize the results [@doi:10.1093/bib/bbn043].
This manual curation process requires a significant amount of effort and time: in 2007 researchers estimated that filling in the missing annotations would require approximately 8.4 years [@doi:10.1093/bioinformatics/btm229].
The rate of publications has continued to increase exponentially [@doi:10.1002/asi.23329].
This has been recognized as a considerable challenge, which can lead to gaps in knowledge bases [@doi:10.1093/bioinformatics/btm229].  
Relationship extraction has been studied as a solution towards handling this problem [@doi:10.1093/bib/bbn043].
This process consists of creating a machine learning system to automatically scan and extract relationships from textual sources.
Machine learning methods often leverage a large corpus of well-labeled training data, which still requires manual curation.
Distant supervision is one technique to sidestep the requirement of well-annotated sentences: with distant supervision one makes the assumption that all sentences containing an entity pair found in a selected database provide evidence for a relationship [@doi:10.3115/1690219.1690287].
Distant supervision provides many labeled examples; however it is accompanied by a decrease in the quality of the labels.  
Ratner et al. [@arxiv:1605.07723] recently introduced "data programming" as a solution.
Data programming combines distant supervision with the automated labeling of text using hand-written label functions.
The distant supervision sources and label functions are integrated using a noise aware generative model that is used to produce training labels.
Combining distant supervision with label functions can dramatically reduce the time required to acquire sufficient training data.
However, constructing a knowledge base of heterogeneous relationships through this framework still requires tens of hand-written label functions for each relationship type.
Writing useful label functions requires significant error analysis, which can be a time-consuming process.  

In this paper, we aim to address the question: to what extent can label functions be re-used across different relationship types?
We hypothesized that sentences describing one relationship type may share information in the form of keywords or sentence structure with sentences that indicate other relationship types.
We designed a series of experiments to determine the extent to which label function re-use enhanced performance over distant supervision alone.
We examined relationships that indicated similar types of physical interactions (i.e., gene-binds-gene and compound-binds-gene) as well as different types (i.e., disease-associates-gene and compound-treats-disease).
The re-use of label functions could dramatically reduce the number required to generate and update a heterogeneous knowledge graph.

### Related Work

Relationship extraction is the process of detecting and classifying semantic relationships from a collection of text.
This process can be broken down into three different categories: (1) the use of natural language processing techniques such as manually crafted rules and heuristics for relationship extraction, (2) the use of unsupervised methods via co-occurrence scores or clustering to find patterns within sentences and documents documents, and (3) supervised or semi-supervised machine learning for classifying the presence of a relation within documents or sentences.
In this section, we discuss selected efforts under each category.

#### Rule Based Extractors

Rule based extractors relies on expert knowledge to perform extraction.
Typically, these systems are created via linguistic rules and heuristics to identify key sentences or phrases.
For example, a relation extractor focused on protein phosphorylation events would identify sentences containing the key phrase "gene X phosphorylates gene Y" [@doi:10.1109/TCBB.2014.2372765].
The word "phosphorylates" is a straightforward indication that both genes play on role in protein phosphorylation.
Other phrase extractor systems were constructed to identify drug-disease treatments [@tag:ctd_medline], pharmcogenomic events [@tag:pharmpresso] and protein-protein interactions [@tag:ppinterfinder; @tag:hpiminer].

Following key phrases, another option is to analyze a sentence's grammatical structure [@tag:pkde4j].
This process is called dependency tree parsing.
Dependency trees are data structures that capture a sentence's grammatical relation structure in the form of nodes and edges.
Nodes represent words and the edges represent the dependency type shared between two words.
Using these trees, an extractor would classify sentences that share a given dependency pattern as a positive cases for extraction [@tag:pkde4j].
Both approaches provide highly precise results, but are limited by the high specificity of each developed rule.
An extractor may overlook a key sentence or phrase, because the developed rules have never seen this sentence or phrase before.
Because of this limitation, recent approaches have incorporated other methods such as co-occurrence and machine learning systems to improve performance [@doi:10.1186/s12859-018-2103-8; @doi:10.1093/nar/gkx462].
For this project, we took a blind approach to construct our label functions; however, previous work in this section provide substantial inspiration for novel label functions in the future.

#### Unsupervised Extractors

Unsupervised extractors identify key relationships without the need of annotated text.
Notable approaches exploit the fact that two entities may occur together in text.
This event is commonly known as co-occurrence.
Approaches that use co-occurrence typically involve calculating the frequency entity pairs in the text and generating statistics to determine if an entity pair occurs more often than chance.
Entity pairs that score higher than a selected threshold are considered positive cases within this approach.
Extractors in this category have been used to establish disease-gene relationships [@tag:diseases; @tag:polysearch; @tag:dg_text_pubmed; @tag:lgscore; @tag:full_text_co_abstracts; @tag:copub_discovery], protein-protein interactions [@tag:protein_protein_co_network; @doi:10.1093/database/bau012; @tag:full_text_co_abstracts], drug-disease treatments [@tag:abc_drugs], and tissue-gene relations [@doi:10.7717/peerj.1054].
A more recent approach incorporated a machine learning classifier before calculating entity frequencies [@tag:cocoscore].
This classifier used distant superivision to weakly label each training sentence before score every sentence in their corpus.
Each entity pair frequency was score using predicted sentences from the classifier above [@tag:cocoscore].
We compared our results to this approach to measure how well our method performs relative to other methods.

Clustering provides a unique way to identify relationships from text.
A recent approach used bi-clustering to identify key sentence groups from Pubmed abstracts [@tag:global_network].
A dependency tree was generate for every sentence in each abstract. 
Once genereated dependency trees were clustered together based on similarity.
Each generated cluster was manually curated to establish relations groups [@tag:global_network].
This approach took a unique look and we incorporated their results as our domain heuristic label functions.
Overall, unsupervised approaches do not rely on well-annotated training data and provide excellent recall; however, performance is limited in terms of precision when compared to supervised methods [@doi:10.1038/nrg1768; @doi:10.1016/j.ymeth.2015.01.015].

#### Supervised Extractors

Supervised extraction consists of using classification labels, positive and negative,to train machine learning algorithms to predict the existence of a relationship.
Generally, these datasets are artifically created via some form of manual curation [@tag:befree; @tag:eu_adr; @tag:comagc; @tag:craft; @tag:aimed; @tag:bioinfer; @raw:LLL; @tag:hprd50; @raw:IEPA].
Shared tasks have opened the door to quickly create these datasets [@tag:biocreative_v; @raw:biocreative/chemprot; @doi:10.1186/1471-2105-9-S3-S6].

The BioCreative VI track 5 task focused on classifying compound-protein interactions and has led to a great deal of work on the topic .
These datasets are used equally among studies, but can generate noticeable  differences in terms of performance [@doi:10.1186/1471-2105-9-S3-S6].
Recent work with supervised machine learning methods has often focused on compounds that induce a disease: an important question for toxicology and the subject of the BioCreative V dataset.
We don't consider environmental toxicants in our work, as our source databases for distant supervision are primarily centered around FDA-approved therapies.
Curators manually annotated 2,432 PubMed abstracts for five different compound protein interactions (agonist, antagonist, inhibitor, activator and substrate/product production) as part of the BioCreative task. 
The best performers on this task achieved an F1 score of 64.10% [@raw:biocreative/chemprot].

Support vector machines have been repeatedly used to detect DaG relationships [@tag:befree; @tag:dtminer; @tag:ensemble_svm;@tag:ppi_graph_kernels; @tag:protein_docking; @tag:limtox; @tag:lptk;].
These models perform well in large feature spaces, but are slow to train as the number of data points becomes asymptotically large.
Recently, some studies have used deep neural network models.
One used a pre-trained recurrent neural network [@tag:biobert], and another used distant supervision [@tag:deep_dive_dag].
Due to the success of these two models, we decided to use a deep neural network as our discriminative model.

However, with the growing popularity of deep learning numerous deep neural network architectures have been applied [@tag:ppi_bilstm; @tag:ppi_deep_conv; @tag:mcdepcnn; @tag:semi_supervised_vae; @tag:biobert; @tag:cbg_ensemble_dl; @tag:cbg_transfer_learning; @tag:cbg_neural_attention; @tag:recursive_nn; @tag:semi_supervised_vae].
Distant supervision has also been used in this domain [@tag:deep_dive], and in fact this effort was one of the motivating rationales for our work.


