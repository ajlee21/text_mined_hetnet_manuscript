# Introduction

Populating knowledge bases is a daunting and arduous task that takes a significant amount of manual effort to accomplish.
Typically, this task involves  manual curation, which is the process of curators reading and parsing published papers to extract relationships of interest. 
This framework becomes infeasible as the rate of publications continues to increase exponentially [@doi:10.1002/asi.23329].
A common solution towards this problem is to undergo relationship extraction.
Relationship extraction is the process of creating a machine learning system to automatically scan and extract relationships from textual sources.
The caveat towards this solution is that these systems need a non-trivial amount of training examples to work well.
To generate these examples involves the arduous process of hand-labeling each example one by one.  
A common technique to ease this process is to use distant supervision, which makes the assumption that that all sentences, that contain an entity pair found in a well established database, will provide evidence for a desired relationship [@doi:10.3115/1690219.1690287].
This technique is useful for quickly generating training examples; however, it generates a lot of noise i.e. a signficiant increase in the number of false positives.  
Recently, there has been work that builds off of this idea called the data programming paradigm [@arxiv:1605.07723].
This paradigm generates large annotated datasets quickly by using weak supervision strategies in the form of label functions and train a noise aware model to incorporate these strategies.
Empirically, creating a useful label function under this paradigm can take a significant amount of time.
In this paper, we aim to answer the question: can we accelerate the label function generation process by reusing label functions across different relationship types?

## Recent Work

Talk about what has been done in the field in regards to text mining and knowledge base integration
