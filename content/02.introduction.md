## Introduction

Knowledge bases are important resources that hold complex structured and unstructed information. 
These resources have been used in important tasks such as network analysis for drug repurposing discovery [@doi:10.1371/journal.pone.0084912; @doi:10.1101/385617; @doi:10.7554/eLife.26726] or as a source of training labels for text mining systems [@doi:10.3115/1690219.1690287; @doi:10.1101/444398; @doi:10.1186/s12859-019-2873-7]. 
Populating knowledge bases often requires highly-trained scientists to read biomedical literature and summarize the results [@doi:10.1093/bib/bbn043].
This manual curation process requires a significant amount of effort and time: in 2007 researchers estimated that filling in the missing annotations would require approximately 8.4 years [@doi:10.1093/bioinformatics/btm229].
The rate of publications has continued to increase exponentially [@doi:10.1002/asi.23329].
This has been recognized as a considerable challenge, which can lead to gaps in knowledge bases [@doi:10.1093/bioinformatics/btm229].  
Relationship extraction has been studied as a solution towards handling this problem [@doi:10.1093/bib/bbn043].
This process consists of creating a machine learning system to automatically scan and extract relationships from textual sources.
Machine learning methods often leverage a large corpus of well-labeled training data, which still requires manual curation.
Distant supervision is one technique to sidestep the requirement of well-annotated sentences: with distant supervision one makes the assumption that all sentences containing an entity pair found in a selected database provide evidence for a relationship [@doi:10.3115/1690219.1690287].
Distant supervision provides many labeled examples; however it is accompanied by a decrease in the quality of the labels.  
Ratner et al. [@arxiv:1605.07723] recently introduced "data programming" as a solution.
Data programming combines distant supervision with the automated labeling of text using hand-written label functions.
The distant supervision sources and label functions are integrated using a noise aware generative model that is used to produce training labels.
Combining distant supervision with label functions can dramatically reduce the time required to acquire sufficient training data.
However, constructing a knowledge base of heterogeneous relationships through this framework still requires tens of hand-written label functions for each relationship type.
Writing useful label functions requires significant error analysis, which can be a time-consuming process.  

In this paper, we aim to address the question: to what extent can label functions be re-used across different relationship types?
We hypothesized that sentences describing one relationship type may share information in the form of keywords or sentence structure with sentences that indicate other relationship types.
We designed a series of experiments to determine the extent to which label function re-use enhanced performance over distant supervision alone.
We examined relationships that indicated similar types of physical interactions (i.e., gene-binds-gene and compound-binds-gene) as well as different types (i.e., disease-associates-gene and compound-treats-disease).
The re-use of label functions could dramatically reduce the number required to generate and update a heterogeneous knowledge graph.

### Related Work

Relationship extraction is the process of detecting and classifying semantic relationships from a collection of text.
This process can be broken down into three different categories: (1) the use of natural language processing techniques such as manually crafted rules and the identification of key text patterns for relationship extraction, (2) the use of unsupervised methods via co-occurrence scores or clustering, and (3) supervised or semi-supervised machine learning using annotated datasets for the classification of documents or sentences.
In this section, we discuss selected efforts for each category.

#### Rule Based Extractors

Rule based extraction consists of experts devising simple rules and heuristics to identify relationships.
These rules range from simple textual patterns [@doi:10.1186/s12859-018-2103-8; @doi:10.1186/1471-2105-14-181;
@doi:10.1093/nar/gkx462; @doi:10.1109/TCBB.2014.2372765; @tag:ctd_medline; @tag:pharmpresso] and strings of keywords to using the grammatical structure of sentences [@tag:pkde4j].

Briefly describe one or two of these publications.
@tag:ppinterfinder; -> uses @tag:hpiminer;


#### Unsupervised Extractors

Unsupervised extracts are designed to identify key relationships without the need of classification labels.
Notable approaches utilize indicative events that two entites may share a relationship if they occur together more often than chance in text.
This circumstance is referred to as co-occurrence.
Pletscher-Frankild et al. used a co-occurrence approach to establish assocications between diseases and genes [@tag:diseases].
This approach calculated the frequency entites appeared together as well as individually and then calculated an odds ratio to determine if two entities appeared together more than chance [@tag:diseases].
Besides disease gene associations, this approach has been applied to establish protein-protein interactions [@tag:string].
The above approach was modified to incorproate a distantly supervised classifier to update scoring scheme with sentence context information [@tag:cocoscore].
We compared our results to this approach to measure how well our overall method performs relative to other methods.

Following abtracts, Westergaard et al. used a co-occurence approach to mine full text [@tag:full_text_co_abstracts]. 
They report that full text outperformed baseline models [@tag:full_text_co_abstracts], which is rationale for future approaches to consider full text as well as abstracts.
Similar co-occurence approaches [@tag:dg_text_pubmed;@tag:lgscore; @tag:polysearch; @tag:protein_protein_co_network] ranged from calculating frequency on a sentence vs abstract level to including other features such as term weight.

Following co-occurece, other approaches incorporated an inference model that uses semantic inference to establish a relationship between entities [@tag:copub_discovery; @tag:abc_drugs].
For example, compound X affects Genes Y while Gene Y is upregulated in Disease Z; therefoere Compound X shares a relationship with Disease Z.
This method depends on prior information, which presents a challenge for detecting under-studied relationships.
One study used clustering to determine biomedical relationships.
This bi-clustering approach to detect DaG-relevant sentences from Pubmed abstracts [@tag:global_network] with clustering of dependency paths grouping similar sentences together.
The results of this work is incorporated as our domain heuristic label functions.

Overall, unsupervised approaches do not rely on a well-annotated training performance and tend to provide excellent recall, though the precision is often worse than with supervised methods [@doi:10.1038/nrg1768; @doi:10.1016/j.ymeth.2015.01.015].


#### Supervised Extraction

Supervised extraction consists of using classification labels, positive and negative,to train machine learning algorithms to predict the existence of a relationship.
Generally, these datasets are artifically created via some form of manual curation [@tag:befree; @tag:eu_adr; @tag:comagc; @tag:craft; @tag:aimed; @tag:bioinfer; @raw:LLL; @tag:hprd50; @raw:IEPA].
Shared tasks have opened the door to quickly create these datasets [@tag:biocreative_v; @raw:biocreative/chemprot; @doi:10.1186/1471-2105-9-S3-S6].

The BioCreative VI track 5 task focused on classifying compound-protein interactions and has led to a great deal of work on the topic .
These datasets are used equally among studies, but can generate noticeable  differences in terms of performance [@doi:10.1186/1471-2105-9-S3-S6].
Recent work with supervised machine learning methods has often focused on compounds that induce a disease: an important question for toxicology and the subject of the BioCreative V dataset.
We don't consider environmental toxicants in our work, as our source databases for distant supervision are primarily centered around FDA-approved therapies.
Curators manually annotated 2,432 PubMed abstracts for five different compound protein interactions (agonist, antagonist, inhibitor, activator and substrate/product production) as part of the BioCreative task. 
The best performers on this task achieved an F1 score of 64.10% [@raw:biocreative/chemprot].

Support vector machines have been repeatedly used to detect DaG relationships [@tag:befree; @tag:dtminer; @tag:ensemble_svm;@tag:ppi_graph_kernels; @tag:protein_docking; @tag:limtox; @tag:lptk;].
These models perform well in large feature spaces, but are slow to train as the number of data points becomes large.
Recently, some studies have used deep neural network models.
One used a pre-trained recurrent neural network [@tag:biobert], and another used distant supervision [@tag:deep_dive_dag].
Due to the success of these two models, we decided to use a deep neural network as our discriminative model.

However, with the growing popularity of deep learning numerous deep neural network architectures have been applied [@tag:ppi_bilstm; @tag:ppi_deep_conv; @tag:mcdepcnn; @tag:semi_supervised_vae; @tag:biobert; @tag:cbg_ensemble_dl; @tag:cbg_transfer_learning; @tag:cbg_neural_attention; @tag:recursive_nn; @tag:semi_supervised_vae].
Distant supervision has also been used in this domain [@tag:deep_dive], and in fact this effort was one of the motivating rationales for our work.


