#Materials and Methods

##Dataset
Talk about dataset - Pubtator
Talk about preprocessing Pubtator
Talk about hand annotations for each realtion

## Label Functions
describe what a label function is and how many we created for each relation

## Training Models
### Generative Model
talk about generative model and how it works
### Word Embeddings
mention facebooks fasttext model and how we used it to train word vectors
### Discriminator Model
talk about the discriminator model and how it works
### Discriminator Model Calibration
talk about calibrating deep learning models with temperature smoothing

## Experimental Design
Being able to re-use label functions across edge types would substantially reduce the number of label functions required extract multiple relationship types from the biomedical literature.
We first established a baseline by training a generative model using only distant supervision label functions for the target edge type.
As an example, for the gene-interacts-gene edge type we used label functions that returned a `1` if the pair of genes was included in (LIST THE DATABASES HERE).
We then compared models that also included text and domain-heuristic label functions to these.
We sampled with replacement from these text- and domain-heuristic label functions separately within edge types, across edge types, and from a pool of all label functions.
We compared within-edge-type performance to across-edge-type and all-edge-type performance.
We performed each sampling for XYZ evenly-spaced numbers of label functions for each comparison from one to the total number of possible label functions.
We repeated the sampling process 50 times for each point.
We evaluated both generative and discriminative models at each point, and we recorded the performance of each in terms of the area under the receiver operating characteristic curve (AUROC) and the area under the precision-recall curve (AUPR).
For each relationship we trained a generative model using only label functions in the database category.
These models surve as our baseline.
Following the baseline models, we randomly sample a fixed number of label functions (five evenly spaced points between one and the total number of constructed functions) from both the text patterns and domain heuristics categories.
These sampled functions are incorporated on top of the baseline models.
We record performance of each sample in terms of area under the receiver operating characteristic curve (AUROC).
Following the generative model, we trained a discriminator model to build off of the generated labels produced from each sampled run.
The performance of the discriminator model is reported in terms of AUROC as well.
We repeat this sampling process 50 times for each specified sampled size.
