#Materials and Methods

##Dataset
Talk about dataset - Pubtator
Talk about preprocessing Pubtator
Talk about hand annotations for each realtion

## Label Functions for Annotating Sentences
We used simple pythonic functions called label functions to quickly annotate our candidates sentences.
These functions emit: a positive label (1), a negative label (-1) 
or abstain from emitting a label (0).
We constructed three categories of label functions: databases, text patterns and domain heuristics.
These categories are described below using the following candidate sentence as an example: \"<span style="color:blue">PTK6</span> may be a novel therapeutic target for <span style="color:brown">pancreatic cancer</span>."

**Databases**: Label functions in this category return a result based on whether or not a relationship has been established between the co-mention pairs in a certain database.
They emit a positive label if a relationship between the co-mentioned pair exists within the database.
If there is no relationship, they abstain.
If the label function abstains, another label function will emit a negative label.

$$ \Lambda_{DB}(\color{#875442}{D}, \color{#02b3e4}{G}) = 
\begin{cases}
 1 & (\color{#875442}{D}, \color{#02b3e4}{G}) \in DB \\
0 & otherwise \\
\end{cases} $$

$$ \Lambda_{\neg DB}(\color{#875442}{D}, \color{#02b3e4}{G}) = 
\begin{cases}
 -1 & (\color{#875442}{D}, \color{#02b3e4}{G}) \notin DB \\
0 & otherwise \\
\end{cases} $$

**Text Patterns**: These label functions are designed to provide information about the relationship of the entities in the sentence such as whether keywords are found in the sentence, the number of words between two mentions and rules on grammatical structure of a sentence.
These functions emit a positive or negative output depending on the situation.
In general, those focused on keywords emit positives and those focused on negation emit negatives.

$$ \Lambda_{TP}(\color{#875442}{D}, \color{#02b3e4}{G}) = 
\begin{cases}
 1 & "target" \in Candidate \> Sentence \\
 0 & otherwise \\
\end{cases} $$

**Domain Heuristics**: This category of label functions uses domain-specific heuristic rules to annotate sentences.
We used Percha et al.[@doi:10.1093/bioinformatics/bty114] as this category's source of label functions.
If a sentence was highlighted in their study, our label functions emitted a positive label.
$$
\Lambda_{DH}(\color{#875442}{D}, \color{#02b3e4}{G}) = \begin{cases}
    1 & Candidate \> Sentence \in Experimental \> Results\\
    0 & otherwise \\
    \end{cases}
$$

Roughly half of our label functions are based on text patterns, while the others are distributed across the databases and domain heuristics (Table 2).

| Relationship | Databases (DB) | Text Patterns (TP) | Domain Heuristics (DH) |
| --- | :---: | :---: | :---: |
| Disease associates Gene (DaG) | 7 | 20 | 10 | 
| Compound treats Disease (CtD) | 3 | 15 | 7 |
| Compound binds Gene (CbG) | 9 | 13 | 7 | 
| Gene interacts Gene (GiG) | 9 | 20 | 8 | 

Table 2. The distribution of each label function per relationship. 

## Training Models
### Generative Model
talk about generative model and how it works
### Word Embeddings
mention facebooks fasttext model and how we used it to train word vectors
### Discriminator Model
talk about the discriminator model and how it works
### Discriminator Model Calibration
talk about calibrating deep learning models with temperature smoothing

## Experimental Design
talk about sampling experiment
