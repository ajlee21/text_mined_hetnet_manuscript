#Materials and Methods

##Dataset
Talk about dataset - Pubtator
Talk about preprocessing Pubtator
Talk about hand annotations for each realtion

## Label Functions
describe what a label function is and how many we created for each relation

## Training Models
### Generative Model
talk about generative model and how it works
### Word Embeddings
To allow our machine learning system to use text, we train word embeddings to encode sentences from medline. 
We used facebook's fasttext [@arxiv:1607.04606] to generate word embeddings. 
Using a modified skipgram model[@arxiv:1301.3781], each word is treated as a bag of n-gram characters.
The weights for the given n-gram model are optimized by predicting the context given the candiadte n-gram. 
Lastly, the vectors for each n-gram model are adding to have a unifed vector representing the total word itself.
We trained the fasttext model for 20 epochs using a window size of 2 and generated word embeddings of size 300. 
For each relationship we trained word embeddings using all sentences that contained the relevant mention pair.

### Discriminator Model
talk about the discriminator model and how it works
### Discriminator Model Calibration
talk about calibrating deep learning models with temperature smoothing

## Experimental Design
talk about sampling experiment
