#Materials and Methods

##Dataset
Talk about dataset - Pubtator
Talk about preprocessing Pubtator
Talk about hand annotations for each realtion

## Label Functions for Annotating Sentences
We used simple pythonic functions called label functions to quickly annotate our candidates sentences.
These functions are designed to emit: a positive label (1), a negative label (-1) 
or abstain from emitting a label (0).
We constructed label functions in the following three categories (Databases, Text Patterns and Domain Heuristics).
These categories are described below using the following candidate sentence as an example: \"<span style="color:blue">PTK6</span> may be a novel therapeutic target for <span style="color:brown">pancreatic cancer</span>."

**Databases**: This category of label functions are designed to utilize publicly available databases to determine if a co-mention pair has been previously established.
It emits a positive label if the co-mention pair is located within a database, otherwise the given label function will abstain.
If the label function abstains, another label function will emit a negative label.

$$ \Lambda_{DB}(\color{#875442}{D}, \color{#02b3e4}{G}) = 
\begin{cases}
 1 & (\color{#875442}{D}, \color{#02b3e4}{G}) \in DB \\
0 & otherwise \\
\end{cases} $$

$$ \Lambda_{\neg DB}(\color{#875442}{D}, \color{#02b3e4}{G}) = 
\begin{cases}
 -1 & (\color{#875442}{D}, \color{#02b3e4}{G}) \notin DB \\
0 & otherwise \\
\end{cases} $$

**Text Patterns**: This category of label functions are designed to provide contextual information such as keywords, number of words between two mentions and rules on grammatical structure of a sentence.
The polarity of these functions can vary between positive and negative depending on the situation.

$$ \Lambda_{TP}(\color{#875442}{D}, \color{#02b3e4}{G}) = 
\begin{cases}
 1 & "target" \in Candidate \> Sentence \\
 0 & otherwise \\
\end{cases} $$

**Domain Heuristics**: This category of label functions is designed to incorporate domain expert knowledge and heuristic rules to annotation sentences.
We used the findings of Percha et al.[@doi:10.1093/bioinformatics/bty114] to generate label functions of this category.
If a sentence was highlighted in their study, our label functions emitted a positive label.
$$
\Lambda_{DH}(\color{#875442}{D}, \color{#02b3e4}{G}) = \begin{cases}
    1 & Candidate \> Sentence \in Experimental \> Results\\
    0 & otherwise \\
    \end{cases}
$$

After constructing these label functions we ended up with the following distribution (shown in Table 2).

| Relationship | Databases (DB) | Text Patterns (TP) | Domain Heuristics (DH) |
| --- | :---: | :---: | :---: |
| Disease associates Gene (DaG) | 7 | 20 | 10 | 
| Compound treats Disease (CtD) | 3 | 15 | 7 |
| Compound binds Gene (CbG) | 9 | 13 | 7 | 
| Gene interacts Gene (GiG) | 9 | 20 | 8 | 

Table 2. The distribution of each label function per relationship. 
Some relationships take more label functions than others to achieve acceptable performance.

## Training Models
### Generative Model
talk about generative model and how it works
### Word Embeddings
mention facebooks fasttext model and how we used it to train word vectors
### Discriminator Model
talk about the discriminator model and how it works
### Discriminator Model Calibration
talk about calibrating deep learning models with temperature smoothing

## Experimental Design
talk about sampling experiment
