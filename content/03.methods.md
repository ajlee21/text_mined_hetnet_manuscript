#Materials and Methods

##Dataset
Talk about dataset - Pubtator
Talk about preprocessing Pubtator
Talk about hand annotations for each realtion

## Label Functions
describe what a label function is and how many we created for each relation

## Training Models
### Generative Model
talk about generative model and how it works
### Word Embeddings
mention facebooks fasttext model and how we used it to train word vectors
### Discriminative Model
The discriminative model is a neural network, which we train to predict labels from the generative model.
The expectation is that the discriminative model can learn more complete features of the text than the label functions that are used in the generative model.
We used a convolutional neural network with multiple filters as our discriminative model.
This network uses multiple filters with fixed widths of 300 dimensions and can have varying heights (Figure {@fig:convolutional_network}).
For our model we each filter at a fixed height of 7, because this provided the best performance in terms of relationship classification [@arxiv:1510.03820].
We trained this model for 20 epochs using the adam optimizer [@arxiv:1412.6980] with a learning rate of 0.001.
This optimizer used pytorch's default parameter settings.
We added a L2 penalty on the network weights to prevent overfitting.
Lastly, we added a dropout layer (p=0.25) between the fully connected layer and the softmax layer.

![
The architecture of the discriminative model is a convolutional neural network. We perform a convolution step using multiple filters. 
These filters generate a feature map that is sent into a maximum pooling layer. 
This layer extracts the largest feature in each of these maps. The extract features are concatenated into a singular vector that will be passed into a fully connected network. 
The fully connected network has 300 neurons for the first layer, 100 neurons for the second layer and 50 neurons for the last layer. 
From the fully connected network the last step is to generate predictions using the softmax layer.
](images/figures/convolutional_neural_network/convolutional_neural_nework.png){#fig:convolutional_network}


### Calibration of the Discriminative Model
Often many tasks require a machine learning model to output reliable probability predictions. 
A model is well calibrated if the probabilities emitted from the model match the observed probabilities: a well-calibrated model that assigns a class label with 80% probability should have that class appear 80% of the time.
Typically, deep learning models tend to be poorly calibrated [@arxiv:1706.04599; @arxiv:1807.00263].
These models are usually over-confidenent in their predictions.
As a result, we calibrated our convolutional neural network using temperature scaling. 
Temperature scaling uses a parameter T to scale each value of the logit vector (z) before being passed into the softmax (SM) function.

$$\sigma_{SM}(\frac{z_{i}}{T}) = \frac{\exp(\frac{z_{i}}{T})}{\sum_{i}\exp(\frac{z_{i}}{T})}$$

We found the optimial T by minimizing the negative log likelihood (NLL) of a held out validation set.
The benefit of using this method is the model becomes more reliable and the accuracy of the model doesn't change [@arxiv:1706.04599].

## Experimental Design
talk about sampling experiment
