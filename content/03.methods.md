#Materials and Methods

##Dataset
Talk about dataset - Pubtator
Talk about preprocessing Pubtator
Talk about hand annotations for each realtion

## Label Functions
describe what a label function is and how many we created for each relation

## Training Models
### Generative Model
talk about generative model and how it works
### Word Embeddings
Word embeddings are representations that map individual words to k-dimensional real valued vectors.
These embeddings have been shown to capture the semantic and syntatic information between words very well [@arxiv:1310.4546].
Using all candidate sentences for each individual relationship pair, we trained facebook's fastText [@arxiv:1607.04606] to generate word embeddings.
This model was trained for 20 epochs using a window size of 2 and generated word embeddings of size 300. 
The fastText model uses a skipgram model that aims to predict the context given a candidate word [@arxiv:1301.3781]. 
Unlike the regular skipgram model, the fasttext model uses a novel scoring function that treats each word as a bag of character n-grams.
Once optimized, the word embeddings are used to train the discriminator model. 

### Discriminator Model
talk about the discriminator model and how it works
### Discriminator Model Calibration
talk about calibrating deep learning models with temperature smoothing

## Experimental Design
talk about sampling experiment
