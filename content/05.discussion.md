## Discussion

We tested the feasibility of re-using label functions to undergo relationship extraction.
As expected some relationships are easier to predict than others.
In respect to compound treats disease (CtD) we were able to achieve a signficantly high area under the receiver operating curve.
We can base this result because of the simplistic and direct language scientist use when talking about disease treatments.
In respect disease associates gene (DaG) this broad edge type has a lot of open language and keywords. 
For example a gene could be upregulated or downregulated in a disease setting, which leads towards the inference of association or a gene could be found within an association study. 
This explains the increase in prediction performance when label functions are sampled from any edge type (not just DaG). ({@fig:generative_model_auroc}, {@fig:generative_model_auprc}).  
We found that adding relevant label functions increases in performance.
This can been seen in the on-diagonals of generative model performance plots ({@fig:generative_model_auroc}, {@fig:generative_model_auprc}).
An interesting thing to note is that performance for all edge types greatly increases when one adds text patterns on top of the distant supervision label functions. 
This could phenomonon could arise from the lack of coverage some of these curated databases suffer from.
A strange phenomon is that performance for every edge type increases when one label functions is randomly sampled.
<span style="color:red">This leads us to believe that the generative model can benefit from have a small amount of noise incorporated into the model.  <- I think this might have been debunked.</span>
Any other hypothesis for performance spike here...  
For the discriminative model performance is heavily correlated with the generative model's performance.
As the generative model's performance increases or decreases the discriminative model follows. 
This can be attributed towards the discrimintive model overfitting to the generative model. 
In regards to overfitting this can be seen in the off diagonals of the discrminative model performance (Figure {@fig:discriminative_model_auroc} and {@fig:discriminative_model_auprc}).  
For model calibration process the discriminative model calibrates pretty well for DaG and CtD.
Coincidently, these two edge types have high performance in both AUROC and AUPR.
CbG and GiG have poor calibration.
This is due to a class imbalance for both edge types as only 7% and 12% of annotated sentenes were labeled positive (Table {@tbl:candidate-sentences}).
Lastly, our model had comparable perofrmance with the CoCoScore model.
Although our model didn't outperform the other model, it wasnt outmatched. 
This shows feasibility that our model can effectively parse out sentences and edges with decent amount of accuracy. 
Also a reason for under performance is that Hetionet v1 is lacking in coverage which can result in our model being penalized for edges that haven't been recognized by Hetionet.
