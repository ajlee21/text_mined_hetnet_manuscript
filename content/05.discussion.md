## Discussion

We tested the feasibility of re-using label functions to extract relationships from literature.
Through our sampling experiment we found that adding relevant label functions increases prediction performance (shown in the on-diagonals of Figures {@fig:generative_model_auroc} and {@fig:generative_model_auprc}).
We found that label functions designed from relatively related edge types can increase performance (seen when GiG label functions predicts CbG and vise versa).
We also found that one edge type (DaG) is agnositic to label function source (Figures {@fig:generative_model_auroc} and {@fig:generative_model_auprc}). 
An interesting finding is performance increases when adding a single label funciton to the generative model baseline.
Initially we thought that adding a bit of noise aided the model, but this turns out to not be the case (Figure {@fig:random_label_function_auroc} and {@fig:random_label_function_auprc}).
This leads to the lingering question: why does performance drastically increase when adding a single label function to the distant supervision baseline?

The discriminative model didn't work as intended. 
Majority of the time the discriminative model underperformed the generative model (Figures {@fig:discriminative_model_auroc} and {@fig:discriminative_model_auprc}).
Potential reasons for this is the discriminative model overfits to the generative model's predictions and there is a negative class bias for our datasets (Table {@tbl:candidate-sentences}).
The negative bias can skew our results, but overall the discriminative model requires more tuning and regularization to increase performance.

Deep learning models have a tendency to be overconfident in their predictions.
To account for this problem we calibrated our discriminative model.
Despite our efforts, model calibration didn't help performance (Figure {@fig:discriminative_model_calibration}).
This happens because there is not enough hand labeled data for an accurate calibration (Table {@tbl:candidate-sentences}).
The discriminative model overfitting the generative model is a factor in poor calibration performance.
Poor calibration explains why it is hard to recall a high number of exisiting edges (Figure {@fig:hetionet_reconstruction}).
Despite poor calibration our model performed similar performance as a published baseline model (Figure {@fig:cocoscore_comparison}).
This implies that with better tuning the discriminative model could have better performance than the baseline model.
