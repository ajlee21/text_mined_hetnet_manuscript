## Results

### Generative Model Using Randomly Sampled Label Functions
![
Grid of AUROC (A) and AUPR (B) scores for each generative model trained on randomly sampled label functions.
The rows depict the relationship each model is trying to predict and the columns are the edge type specific sources from which each label function is sampled.
For example, the top-left most square depicts the generative model predicting DaG sentences, while randomly sampling label functions designed to predict the DaG relationship. 
The square towards the right depicts the generative model predicting DaG sentences, while randomly sampling label functions designed to predict the CtD relationship.
This pattern continues filling out the rest of the grid.
The right most column consists of pooling every relationship specific label function and proceeding as above.
](https://raw.githubusercontent.com/danich1/snorkeling/ee638b4e45717a86f54a2744a813baaa90bc6b84/figures/label_sampling_experiment/transfer_test_set_auroc.png){#fig:auroc_gen_model_performance}

We added randomly sampled label functions to a baseline for each edge type to evaluate the feasibility of label function re-use.
Our baseline model consisted of a generative model trained with only edge-specific distant supervision label functions.
We reported the results in AUPR and AUROC (Figures {@fig:auroc_gen_model_performance} and {@fig:aupr_gen_model_performance}).  
The on-diagonal plots of figures {@fig:auroc_gen_model_performance} and {@fig:aupr_gen_model_performance} show increasing performance when edge-specific label functions are added on top of the  edge-specific baselines.
The CtD edge type is a quintessential example of this trend.
The baseline model starts off with an AUROC score of 52% and an AUPRC of 28%, which increase to 76% and 49% respectively as more CtD label functions are included. 
DaG edges have a similar trend: performance starting off with an AUROC of 56% and AUPR of 41% then increases to 62% and 45% respectively.
Both the CbG and GiG edges have an increasing trend but plateau after a few label functions are added.  

The off-diagonals in figures {@fig:auroc_gen_model_performance} and {@fig:aupr_gen_model_performance} show how performance varies when label functions from one edge type are added to a different edge type's baseline.
In certain cases (apparent for DaG), performance increases regardless of the edge type used for label functions.
In other cases (apparent with CtD), one label function appears to improve performance; however, adding more label functions does not improve performance (AUROC) or decreases it (AUPR).
In certain cases, the source of the label functions appears to be important: the performance of CbG edges decrease when using label functions from the DaG and CtD categories.

Our initial hypothesis was based on the idea that certain edge types capture similar physical relationships and that these cases would be particularly amenable for label function transfer.
For example, CbG and GiG both describe physical interactions.
We observed that performance increased as assessed by both AUROC and AUPR when using label functions from the GiG edge type to predict CbG edges.
A similar trend was observed when predicting the GiG edge; however, the performance differences were small for this edge type making the importance difficult to assess.  
The last column shows increasing performance (AUROC and AUPR) for both DaG and CtD when sampling from all label functions.
CbG and GiG also had increased performance when one random label function was sampled, but performance decreased drastically as more label functions were added.
It is possible that a small number of irrelevant label functions are able to overwhelm the distant supervision label functions in these cases (see Supplemental Figures {@fig:auroc_random_label_function_performance} and {@fig:aupr_random_label_function_performance}).

![
Grid of AUPR scores for each generative model trained on randomly sampled label functions.
The rows depict the relationship each model is trying to predict and the columns are the edge type specific sources from which each label function is sampled.
For example, the top-left most square depicts the generative model predicting DaG sentences, while randomly sampling label functions designed to predict the DaG relationship. 
The square towards the right depicts the generative model predicting DaG sentences, while randomly sampling label functions designed to predict the CtD relationship.
This pattern continues filling out the rest of the grid.
The right most column consists of pooling every relationship specific label function and proceeding as above.
](https://raw.githubusercontent.com/danich1/snorkeling/ee638b4e45717a86f54a2744a813baaa90bc6b84/figures/label_sampling_experiment/transfer_test_set_auprc.png){#fig:aupr_gen_model_performance}

### Discriminative Model Performance

![
Grid of AUROC scores for each discriminative model trained using generated labels from the generative models.
The rows depict the edge type each model is trying to predict and the columns are the edge type specific sources from which each label function was sampled. 
For example, the top-left most square depicts the discriminator model predicting DaG sentences, while randomly sampling label functions designed to predict the DaG relationship.
The error bars over the points represents the standard deviation between sampled runs.
The square towards the right depicts the discriminative model predicting DaG sentences, while randomly sampling label functions designed to predict the CtD relationship.
This pattern continues filling out the rest of the grid.
The right most column consists of pooling every relationship specific label function and proceeding as above.
](https://raw.githubusercontent.com/danich1/snorkeling/ee638b4e45717a86f54a2744a813baaa90bc6b84/figures/label_sampling_experiment/disc_performance_test_set_auroc.png){#fig:auroc_discriminative_model_performance}

In this framework we used a generative model trained over label functions to produce probabilistic training labels for each sentence.
Then we trained a discriminative model, which has full access to a representation of the text of the sentence, to predict the generated labels.
The discriminative model is a convolutional neural network trained over word embeddings (See Methods).
We report the results of the discriminative model using AUROC and AUPR (Figures {@fig:auroc_discriminative_model_performance} and {@fig:aupr_discriminative_model_performance}).  
  
We found that the discriminative model under-performed the generative model in most cases.
Only for the CtD edge does the discriminative model appear to provide performance above the generative model and that increased performance is only with a modest number of label functions.
With the full set of label functions, performance of both models remain similar.
The one or a few mismatched label functions (off-diagonal) improving generative model performance trend is retained despite the limited performance of the discriminative model.

![
Grid of AUPR scores for each discriminative model trained using generated labels from the generative models.
The rows depict the edge type each model is trying to predict and the columns are the edge type specific sources from which each label function was sampled. 
For example, the top-left most square depicts the discriminator model predicting DaG sentences, while randomly sampling label functions designed to predict the DaG relationship.
The error bars over the points represents the standard deviation between sampled runs.
The square towards the right depicts the discriminative model predicting DaG sentences, while randomly sampling label functions designed to predict the CtD relationship.
This pattern continues filling out the rest of the grid.
The right most column consists of pooling every relationship specific label function and proceeding as above.
](https://raw.githubusercontent.com/danich1/snorkeling/ee638b4e45717a86f54a2744a813baaa90bc6b84/figures/label_sampling_experiment/disc_performance_test_set_auprc.png){#fig:aupr_discriminative_model_performance}

### Discriminative Model Calibration

![
Calibration plots for the discriminative model.
A perfectly calibrated model would follow the dashed diagonal line.
The blue line represents the predictions before calibration and the orange line shows predictions after calibration. 
](https://raw.githubusercontent.com/greenelab/snorkeling/master/figures/model_calibration_experiment/model_calibration.png){#fig:discriminative_model_calibration}

Even deep learning models with high precision and recall can be poorly calibrated, and the overconfidence of these models has been noted [@arxiv:1706.04599; @arxiv:1807.00263].
We attempted to calibrate the best performing discriminative model so that we could directly use the emitted probabilities.
We examined the calibration of our existing model (Figure {@fig:discriminative_model_calibration}, blue line).
We found that the DaG and CtG edge types were, though not perfectly calibrated, were somewhat aligned with the ideal calibration lines.
The CbG and GiG edges were poorly calibrated and increasing model certainty did not always lead to an increase in precision.
Applying the calibration algorithm (orange line) did not appear to bring predictions in line with the ideal calibration line, but did capture some of the uncertainty in the GiG edge type.
For this reason we use the measured precision instead of the predicted probabilities when determining how many edges could be added to existing knowledge bases with specified levels of confidence.

### Baseline Comparison

![
Comparion between our model and CoCoScore model [@tag:cocoscore].
We report both model's performance in terms of AUROC and AUPR.
Our model achieves comparable performance against CoCoScore in terms of AUROC.
As for AUPR, CoCoScore consistently outperforms our model except for CtD. 
](https://raw.githubusercontent.com/danich1/snorkeling/0149086785b19f9429c92565d650e9d049c136ff/figures/literature_models/model_comparison.png){#fig:cocoscore_comparison}

Once our discriminator model is calibrated, we grouped sentences based on mention pair (edges).
We assigned each edge the maximum score over all grouped sentences and compared our model's ability to predict pairs in our test set to a previously published baseline model [@tag:cocoscore].
Performance is reported in terms of AUROC and AUPR (Figure {@fig:cocoscore_comparison}).
Across edge types our model shows comparable performance against the baseline in terms of AUROC.
Regarding AUPR, our model shows hindered performance against the baseline.
The exception for both cases is CtD where our model performs better than the baseline.

### Reconstructing Hetionet

![
A scatter plot showing the number of edges (log scale) we can add or recall at specified precision levels. 
The blue depicts edges existing in hetionet and the orange depicts how many novel edges can be added.
](https://raw.githubusercontent.com/danich1/snorkeling/0149086785b19f9429c92565d650e9d049c136ff/figures/edge_prediction_experiment/edges_added.png){#fig:hetionet_reconstruction}

We evaluated how many edges we can recall/add to Hetionet v1 (Figure {@fig:hetionet_reconstruction} and Supplemental Table {@tbl:edge_prediction_tbl}).
In our evaluation we used edges assigned to our test set.
Overall, we can recall a small amount of edges at high precision thresholds.
A key example is CbG and GiG where we recalled only one exisiting edge at 100% precision.
Despite the low recall, we are still able to add novel edges to DaG and CtD while retaining modest precision.
