## Results

### Generative Model Using Randomly Sampled Label Functions
![
Grid of AUROC scores for each generative model trained on randomly sampled label functions.
The rows depict the relationship each model is trying to predict and the columns are the edge type specific sources from which each label function is sampled.
The right most column consists of pooling every relationship specific label function and proceeding as above.
](https://raw.githubusercontent.com/danich1/snorkeling/ee638b4e45717a86f54a2744a813baaa90bc6b84/figures/label_sampling_experiment/transfer_test_set_auroc.png){#fig:auroc_gen_model_performance}

We added randomly sampled label functions to a baseline for each edge type to evaluate the feasibility of label function re-use.
Our baseline model consisted of a generative model trained with only edge-specific distant supervision label functions.
We reported the results in AUROC and AUPR (Figure {@fig:auroc_gen_model_performance} and Supplemental Figure {@fig:aupr_gen_model_performance}).  
The on-diagonal plots of figure {@fig:auroc_gen_model_performance} and supplemental figure {@fig:aupr_gen_model_performance} show increasing performance when edge-specific label functions are added on top of the  edge-specific baselines.
The CtD edge type is a quintessential example of this trend.
The baseline model starts off with an AUROC score of 52% and an AUPRC of 28%, which increase to 76% and 49% respectively as more CtD label functions are included. 
DaG edges have a similar trend: performance starting off with an AUROC of 56% and AUPR of 41% then increases to 62% and 45% respectively.
Both the CbG and GiG edges have an increasing trend but plateau after a few label functions are added.  

The off-diagonals in figure {@fig:auroc_gen_model_performance} and supplemental figure {@fig:aupr_gen_model_performance} show how performance varies when label functions from one edge type are added to a different edge type's baseline.
In certain cases (apparent for DaG), performance increases regardless of the edge type used for label functions.
In other cases (apparent with CtD), one label function appears to improve performance; however, adding more label functions does not improve performance (AUROC) or decreases it (AUPR).
In certain cases, the source of the label functions appears to be important: the performance of CbG edges decrease when using label functions from the DaG and CtD categories.

Our initial hypothesis was based on the idea that certain edge types capture similar physical relationships and that these cases would be particularly amenable for label function transfer.
For example, CbG and GiG both describe physical interactions.
We observed that performance increased as assessed by both AUROC and AUPR when using label functions from the GiG edge type to predict CbG edges.
A similar trend was observed when predicting the GiG edge; however, the performance differences were small for this edge type making the importance difficult to assess.  
The last column shows increasing performance (AUROC and AUPR) for both DaG and CtD when sampling from all label functions.
CbG and GiG also had increased performance when one random label function was sampled, but performance decreased drastically as more label functions were added.
It is possible that a small number of irrelevant label functions are able to overwhelm the distant supervision label functions in these cases (see Figure {@fig:auroc_random_label_function_performance} and Supplemental Figure {@fig:aupr_random_label_function_performance}).

### Random Label Function Generative Model Analysis
![
A grid of AUROC (A) scores for each edge type.
Each plot consists of adding a single label function on top of the baseline model.
This label function emits a positive (shown in blue) or negative (shown in orange) label at specified frequencies, and performance at zero is equivalent to not having a randomly emitting label function.
The error bars represent 95% confidence intervals for AUROC or AUPR (y-axis) at each emission frequency.
](https://raw.githubusercontent.com/danich1/snorkeling/ee638b4e45717a86f54a2744a813baaa90bc6b84/figures/gen_model_error_analysis/transfer_test_set_auroc.png){#fig:auroc_random_label_function_performance}

We observed that including one label function of a mismatched type to distant supervision often improved performance, so we evaluated the effects of adding a random label function in the same setting.
We found that usually adding random noise did not improve performance (Figure {@fig:auroc_random_label_function_performance} and Supplemental Figure {@fig:aupr_random_label_function_performance}).
For the CbG edge type we did observe slightly increased performance via AUPR (Supplemental Figure {@fig:aupr_random_label_function_performance}).
However, performance changes in general were smaller than those observed with mismatched label types.

### Discriminative Model Calibration

![
Calibration plots for the discriminative model.
A perfectly calibrated model would follow the dashed diagonal line.
The blue line represents the predictions after calibration and the green line shows predictions after calibration. 
](https://raw.githubusercontent.com/danich1/snorkeling/86037d185a299a1f6dd4dd68605073849c72af6f/figures/model_calibration_experiment/model_calibration.png){#fig:discriminative_model_calibration}

Even deep learning models with good AUROC and AUPR statistics can be subject to poor calibration.
Typically, these models are overconfident in their predictions [@arxiv:1706.04599; @arxiv:1807.00263].
We attempted to use temperature scaling to fix the calibration of the best performing discriminative models (Figure {@fig:discriminative_model_calibration}).
Before calibration (green lines), our models were aligned with the ideal calibration only when predicting low probability scores (close to 0.25).
Applying the temperature scaling calibration algorithm (blue lines) did not substantially improve the calibration of the model in most cases.
We observed some improvement for the Disease associates Gene (DaG) predictor, which was particularly noticeable for high-confidence predictions.
