## Results

### Generative Model Using Randomly Sampled Label Functions
![
Grid of AUROC scores for each generative model trained on randomly sampled label functions.
The rows depict the relationship each model is trying to predict and the columns are the edge type specific sources from which each label function is sampled.
The right most column consists of pooling every relationship specific label function and proceeding as above.
](https://raw.githubusercontent.com/danich1/snorkeling/ee638b4e45717a86f54a2744a813baaa90bc6b84/figures/label_sampling_experiment/transfer_test_set_auroc.png){#fig:auroc_gen_model_performance}

We added randomly sampled label functions to a baseline for each edge type to evaluate the feasibility of label function re-use.
Our baseline model consisted of a generative model trained with only edge-specific distant supervision label functions.
We reported the results in AUROC and AUPR (Figure {@fig:auroc_gen_model_performance} and Supplemental Figure {@fig:aupr_gen_model_performance}).  
The on-diagonal plots of figure {@fig:auroc_gen_model_performance} and supplemental figure {@fig:aupr_gen_model_performance} show increasing performance when edge-specific label functions are added on top of the  edge-specific baselines.
The CtD edge type is a quintessential example of this trend.
The baseline model starts off with an AUROC score of 52% and an AUPRC of 28%, which increase to 76% and 49% respectively as more CtD label functions are included. 
DaG edges have a similar trend: performance starting off with an AUROC of 56% and AUPR of 41% then increases to 62% and 45% respectively.
Both the CbG and GiG edges have an increasing trend but plateau after a few label functions are added.  

The off-diagonals in figure {@fig:auroc_gen_model_performance} and supplemental figure {@fig:aupr_gen_model_performance} show how performance varies when label functions from one edge type are added to a different edge type's baseline.
In certain cases (apparent for DaG), performance increases regardless of the edge type used for label functions.
In other cases (apparent with CtD), one label function appears to improve performance; however, adding more label functions does not improve performance (AUROC) or decreases it (AUPR).
In certain cases, the source of the label functions appears to be important: the performance of CbG edges decrease when using label functions from the DaG and CtD categories.

Our initial hypothesis was based on the idea that certain edge types capture similar physical relationships and that these cases would be particularly amenable for label function transfer.
For example, CbG and GiG both describe physical interactions.
We observed that performance increased as assessed by both AUROC and AUPR when using label functions from the GiG edge type to predict CbG edges.
A similar trend was observed when predicting the GiG edge; however, the performance differences were small for this edge type making the importance difficult to assess.  
The last column shows increasing performance (AUROC and AUPR) for both DaG and CtD when sampling from all label functions.
CbG and GiG also had increased performance when one random label function was sampled, but performance decreased drastically as more label functions were added.
It is possible that a small number of irrelevant label functions are able to overwhelm the distant supervision label functions in these cases (see Figure {@fig:auroc_random_label_function_performance} and Supplemental Figure {@fig:aupr_random_label_function_performance}).

### Random Label Function Generative Model Analysis
![
A grid of AUROC (A) scores for each edge type.
Each plot consists of adding a single label function on top of the baseline model.
This label function emits a positive (shown in blue) or negative (shown in orange) label at specified frequencies, and performance at zero is equivalent to not having a randomly emitting label function.
The error bars represent 95% confidence intervals for AUROC or AUPR (y-axis) at each emission frequency.
](https://raw.githubusercontent.com/danich1/snorkeling/ee638b4e45717a86f54a2744a813baaa90bc6b84/figures/gen_model_error_analysis/transfer_test_set_auroc.png){#fig:auroc_random_label_function_performance}

We observed that including one label function of a mismatched type to distant supervision often improved performance, so we evaluated the effects of adding a random label function in the same setting.
We found that usually adding random noise did not improve performance (Figure {@fig:auroc_random_label_function_performance} and Supplemental Figure {@fig:aupr_random_label_function_performance}).
For the CbG edge type we did observe slightly increased performance via AUPR (Supplemental Figure {@fig:aupr_random_label_function_performance}).
However, performance changes in general were smaller than those observed with mismatched label types.

### Text mined edges can expand a database-derived knowledge graph

![
Text-mined edges recreate a substantial fraction of an existing knowledge graph and include new predictions.
This bar chart shows the number of edges we can successfully recall in green and shows the number of new edges that can be added in blue.  
The recall for the Hetionet v1 knowledge graph is shown as a percentage in parentheses.
For example, for the compound treats disease edge our method recalls 85% of existing edges as adds 6,088 new edges.
](https://raw.githubusercontent.com/danich1/snorkeling/6e929e486537c5d105e393e20984b96910c96024/figures/edge_prediction_experiment/edges_added.png){#fig:hetionet_reconstruction}

One of the goals in our work is to measure the extent to which learning multiple edge types could construct a biomedical knowledge graph.
Hetionet v1 is a knowledge graph that was constructed from biomedical databases.
We measured this framework's recall as well as how many new edges could be added with high confidence.
Using a calibrated discriminator model, we scored every candidate sentence within our dataset and grouped candidates based on their mention pair. 
We took the max score within each candidate group and this score represents the probability of the existence of an edge. 
We established edges by using a cutoff score that produced an equal error rate between the false positives and false negatives.
Overall, we were able to recall more than half of preexisting edges for all edge types (Figure {@fig:hetionet_reconstruction}) and report our top ten scoring sentences for each edge type in Supplemental Table {@tbl:edge_prediction_tbl}.
Our best recall is with the Compound treats Disease (CtD) edge type, where we retain 85% of preexisting edges.
Plus, we can add over 6,000 new edges to that category.
In contrast, we could only recall close to 70% of existing edges for the other categories; however, we can add over 40,000 novel edges to each category.
This highlights the fact that Hetionet v1 is missing a compelling amount of biomedical information and this framework is a viable way to close the information gap.
