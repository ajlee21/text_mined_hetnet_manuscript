<style> span.gene_color { color:#02b3e4 } span.disease_color { color:#875442 } span.compound_color { color:#e91e63 } </style> 
# Results

## Random Sampling of Generative Model
place the grid aurocs here for generative model

### Discriminator Model Builds Off Generative Model
In the framework we are using, a generative model is trained over label functions to produce probabilistic training labels for each sentence.
Then we train a discriminative model, which has full access to a representation of the text of the sentence, to predict the generated labels.
The discriminative model is a convolutional neural network trained over word embeddings.
We report the results of the discriminative model using area under the precision recall curve (AUPRC) (Figure {@fig:discriminative_model_auprc}) and area under the receiver operating curve (AUROC) (Figure {@fig:discriminative_model_auroc}).    
Overall the discriminative model performs well when using a generative model that is trained off of edge specific label functions. 
In the top left corner of figures {@fig:discriminative_model_auroc} and {@fig:discriminative_model_auprc} the discriminative model underperforms the generative model when only using distant supervision label functions, but the discriminative model surpasses the generative model when all label functions are included.
This similar trend is seen for the Compound treats Disease (CtD) edge type; however, both the generative and discriminative model perform equally instead of one outcompeting another.
For the Compound binds Gene (CbG) edge type, the discriminative model consistently underperforms the generative model.
Lastly for the Gene interacts Gene (GiG) edge type the discriminative model starts off underperforming the generative model.
Just like with CtD and DaG performance improves as more label functions are added.  
Looking at the off-diagonals in figures {@fig:discriminative_model_auroc} and {@fig:discriminative_model_auprc}, the discriminative model shows variable performance depending on the edge type.
For the DaG row, the discriminative model consistently underperforms the generative model, that is trained using label functions from other edge types. 
The exception towards this trend is when label functions from all edge types are used.
The inverse effect can be seen for CtD row.
The discriminative model starts off out performing the generative model, but then shifts into underperformance as more label functions are added.
For both the CbG and GiG rows performance starts off with an increasing trend, but then either plateaus or sharply declines.
A perfect example of this the described trend can be seen in the last two rows and first two columns.
When one label function is added, performance spikes and then sharply declines as more label functions are added.
Interestingly, the discriminative model does well in predicting the CbG edge type, when using the generative model trained on GiG label functions.
This trend doesn't hold for the reverse as the discriminative model slightly underperforms the generative model.
Lastly for CbG and GiG using all label functions didn't aid in the discriminative model performance.

![
Grid of Area Under the Receiver Operating Curve (AUROC) scores for each discriminative model trained using generated labels from the generative models.
The rows depict the relationship each model is trying to predict and the columns are the relationship specific sources each label function was sampled from. 
For example, the top-left most square depicts the discriminator model predicting Disease associates Gene (DaG) sentences, while randomly sampling label functions designed to predict the DaG relationship. 
The square towards the right depicts the discriminative model predicting DaG sentences, while randomly sampling label functions designed to predict the Compound treats Disease (CtD) relationship.
This pattern continues filling out the rest of the grid.
The last most column consists of pooling every relationship specific label function and proceeding as above.
](https://raw.githubusercontent.com/greenelab/snorkeling/master/figures/label_sampling_experiment/disc_performance_test_set_auroc.png){#fig:discriminative_model_auroc}

![
Grid of Area Under the Receiver Operating Curve (AUROC) scores for each discriminative model trained using generated labels from the generative models.
The rows depict the relationship each model is trying to predict and the columns are the relationship specific sources each label function was sampled from. 
For example, the top-left most square depicts the discriminator model predicting Disease associates Gene (DaG) sentences, while randomly sampling label functions designed to predict the DaG relationship. 
The square towards the right depicts the discriminative model predicting DaG sentences, while randomly sampling label functions designed to predict the Compound treats Disease (CtD) relationship.
This pattern continues filling out the rest of the grid.
The last most column consists of pooling every relationship specific label function and proceeding as above.
](https://raw.githubusercontent.com/greenelab/snorkeling/master/figures/label_sampling_experiment/disc_performance_test_set_auprc.png){#fig:discriminative_model_auprc}

### Discriminative Model Calibration
Even deep learning models with high precision and recall can be poorly calibrated, and the overconfidence of these models has been noted [@arxiv:1706.04599; @arxiv:1807.00263].
We attempted to calibrate the best performing discriminative model so that we could directly use the emitted probabilities.
We examined the calibration of our existing model (Figure {@fig:discriminative_model_calibration}, blue line).
We found that the DaG and CtG edge types were, though not perfectly calibrated, were somewhat aligned with the ideal calibration lines.
The CbG and GiG edges were poorly calibrated and increasing model certainty did not always lead to an increase in precision.
Applying the calibration algorithm (orange line) did not appear to bring predictions in line with the ideal calibration line, but did capture some of the uncertainty in the GiG edge type.
For this reason we use the measured precision instead of the predicted probabilities when determining how many edges could be added to existing knowledge bases with specified levels of confidence.

![
Calibration plots for the discriminative model.
A perfectly calibrated model would show a diagonal line (dashed).
The blue line represents the predictions before calibration and the orange line shows predictions after calibration. 
](https://raw.githubusercontent.com/greenelab/snorkeling/master/figures/model_calibration_experiment/model_calibration.png){#fig:discriminative_model_calibration}

## Reconstructing Hetionet
place figure of number of new edges that can be added to hetionet as well as edges we can reconstruct using this method
