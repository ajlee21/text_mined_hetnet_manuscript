# Results

## Random Sampling of Generative Model
place the grid aurocs here for generative model

### Discriminator Model Builds Off Generative Model
In this framework we used a generative model trained over label functions to produce probabilistic training labels for each sentence.
Then we train a discriminative model, which has full access to a representation of the text of the sentence, to predict the generated labels.
The discriminative model is a convolutional neural network trained over word embeddings.
We report the results of the discriminative model using area under the precision recall curve (AUPR) (Figure {@fig:discriminative_model_auprc}) and area under the receiver operating curve (AUROC) (Figure {@fig:discriminative_model_auroc}).    
Generally, the discriminative model under-performs compared to the generative model.
In the top left corner of figures {@fig:discriminative_model_auroc} and {@fig:discriminative_model_auprc} the discriminative model underperforms the generative model when only using distant supervision label functions.
As the number of label functions increase the discriminative model starts to have similar performance to the generative model.
Compound-treats-Disease (CtD) starts off with the opposite trend.
The discriminative model outcompetes the generative model; however, as more label functions are added performance starts to decline.
For the Compound-binds-Gene (CbG) edge type, the discriminative model consistently underperforms the generative model.
Gene-interacts-Gene (GiG) has a similar pattern to DaG.
The discriminative model starts off underperforming the generative model.
As soon as more label functions are incorporated performance improves to be on par with the generative model.  
 
With respect to the off-diagonals, the discriminative model uses annotations from the generative model, that is trained using label functions other than the targeted edge type (targeted edge type shown on the y-axes of figures {@fig:discriminative_model_auroc} and {@fig:discriminative_model_auprc}). 
Overall, performance for the discriminative model is second-rate. 
For the top row the discriminative model under performs the generative model.
When using CtD label functions the auroc dips as low as 50%.
Similar scores can be seen with CbG and All (Figure {@fig:discriminative_model_auprc}). 
For the second row, there is an inverse effect.
The discriminative model starts off out-performing the generative model, but diminishes as the number of label functions increases.
In the last two rows performance spikes when using one label function, but plateaus or sharply declines as more are added.
Despite the decreasing trend, the discriminative model performs almost on par with the generative model when predicting the CbG edge type using GiG label functions.
The same trend can be found when predicting the GiG edge using CbG label functions.
Lastly only CtD and DaG slightly benefited from using all label functions at once.
For CbG and GiG they have performance increase using one label function then plateaus or declines when more are added.

![
Grid of Area Under the Receiver Operating Curve (AUROC) scores for each discriminative model trained using generated labels from the generative models.
The rows depict the relationship each model is trying to predict and the columns are the relationship specific sources each label function was sampled from. 
For example, the top-left most square depicts the discriminator model predicting Disease associates Gene (DaG) sentences, while randomly sampling label functions designed to predict the DaG relationship. 
The square towards the right depicts the discriminative model predicting DaG sentences, while randomly sampling label functions designed to predict the Compound treats Disease (CtD) relationship.
This pattern continues filling out the rest of the grid.
The last most column consists of pooling every relationship specific label function and proceeding as above.
](https://raw.githubusercontent.com/greenelab/snorkeling/master/figures/label_sampling_experiment/disc_performance_test_set_auroc.png){#fig:discriminative_model_auroc}

![
Grid of Area Under the Receiver Operating Curve (AUROC) scores for each discriminative model trained using generated labels from the generative models.
The rows depict the relationship each model is trying to predict and the columns are the relationship specific sources each label function was sampled from. 
For example, the top-left most square depicts the discriminator model predicting Disease associates Gene (DaG) sentences, while randomly sampling label functions designed to predict the DaG relationship. 
The square towards the right depicts the discriminative model predicting DaG sentences, while randomly sampling label functions designed to predict the Compound treats Disease (CtD) relationship.
This pattern continues filling out the rest of the grid.
The last most column consists of pooling every relationship specific label function and proceeding as above.
](https://raw.githubusercontent.com/greenelab/snorkeling/master/figures/label_sampling_experiment/disc_performance_test_set_auprc.png){#fig:discriminative_model_auprc}

### Discriminative Model Calibration
Even deep learning models with high precision and recall can be poorly calibrated, and the overconfidence of these models has been noted [@arxiv:1706.04599; @arxiv:1807.00263].
We attempted to calibrate the best performing discriminative model so that we could directly use the emitted probabilities.
We examined the calibration of our existing model (Figure {@fig:discriminative_model_calibration}, blue line).
We found that the DaG and CtG edge types were, though not perfectly calibrated, were somewhat aligned with the ideal calibration lines.
The CbG and GiG edges were poorly calibrated and increasing model certainty did not always lead to an increase in precision.
Applying the calibration algorithm (orange line) did not appear to bring predictions in line with the ideal calibration line, but did capture some of the uncertainty in the GiG edge type.
For this reason we use the measured precision instead of the predicted probabilities when determining how many edges could be added to existing knowledge bases with specified levels of confidence.

![
Calibration plots for the discriminative model.
A perfectly calibrated model would show a diagonal line (dashed).
The blue line represents the predictions before calibration and the orange line shows predictions after calibration. 
](https://raw.githubusercontent.com/greenelab/snorkeling/master/figures/model_calibration_experiment/model_calibration.png){#fig:discriminative_model_calibration}

## Reconstructing Hetionet
place figure of number of new edges that can be added to hetionet as well as edges we can reconstruct using this method
