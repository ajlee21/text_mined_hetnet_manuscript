## Results

### Generative Model Using Randomly Sampled Label Functions

Creating label functions is a labor intensive process that can take days to accomplish.
We sought to accelerate this process by measuring the extent to which label functions can be reused.
Our hypothesis was that certain edge types share similar linguistic features such as keywords and/or sentence structure.
This shared characteristic would make certain edge types amenable to label function reuse.
We designed a set of experiments to test this hypothesis on an individual level (edge vs edge) as well as a global level (collective pool of sources). 
We report results in terms of AUROC (Figures {@fig:auroc_gen_model_test_set} and {@fig:auroc_grabbag_gen_model_test_set}) and AUPR (Supplemental Figure {@fig:aupr_gen_model_test_set} and {@fig:aupr_grabbag_gen_model_test_set}).

Performance increases when edge-specific label functions are added to an edge-specific baseline model, while label function reusability shows modest results.
The quintessential example of the overarching trend is the Compound treats Disease (CtD) edge type, where edge-specific label functions always outperformed transferred label functions.
However, there are hints of label function transferability for selected edge types and label function sources. 
Performance increases as more CbG label functions are incorporated to the GiG baseline model and vise-versa.
This suggests that sentences for GiG and CbG may share similar linguistic features or terminology that allows for label functions to be reused.
Edge-specific Disease associates Gene (DaG) label functions did not improve performance over label functions drawn from other edge types.
Overall, only CbG and GiG show significant signs of reusability which suggests label functions could be shared between the two edge types.

![
Edge-specific label functions are better performing than edge-mismatch label functions but certain mismatch situations show signs of successful transfer.
Each line plot header depicts the edge type the generative model is trying to predict, while the colors represent the source of label functions.
For example orange represents sampling label functions designed to predict the Compound treats Disease (CtD) edge type.
The x axis shows the number of randomly sampled label functions being incorporated onto the database only baseline model (point at 0).
The y axis shows area under the receiver operating curve (AUROC).
Each point on the plot shows the average of 50 sample runs, while the error bars show the 95% confidence intervals of all runs.
The baseline and “All” data points consist of sampling from the entire fixed set of label functions.
](https://raw.githubusercontent.com/danich1/snorkeling/86037d185a299a1f6dd4dd68605073849c72af6f/figures/label_sampling_experiment/transfer_test_set_auroc.png){#fig:auroc_gen_model_test_set}

We found that sampling from all label function sources at once usually underperformed relative to edge-specific label functions (Figure {#fig:auroc_grabbag_gen_model_test_set}).
As more label functions were sampled, the gap between edge-specific sources and all sources widened.
CbG is a prime example of this trend (Figure {#fig:auroc_grabbag_gen_model_test_set}), while CtD and GiG show a similar but milder trend.
DaG was the exception to the general rule: the pooled set of label functions improved performance over the edge-specific ones, which aligns with the previously observed results for individual edge types (Figure {#fig:auroc_gen_model_test_set}).
The decreasing trend when pooling all label functions supports the notion that label functions cannot easily transfer between edge types (exception being CbG on GiG and vise versa).

![
A grid of AUROC (A) scores for each edge type.
Each plot consists of adding a single label function on top of the baseline model.
This label function emits a positive (shown in blue) or negative (shown in orange) label at specified frequencies, and performance at zero is equivalent to not having a randomly emitting label function.
The error bars represent 95% confidence intervals for AUROC or AUPR (y-axis) at each emission frequency.
](https://raw.githubusercontent.com/danich1/snorkeling/ee638b4e45717a86f54a2744a813baaa90bc6b84/figures/gen_model_error_analysis/transfer_test_set_auroc.png){#fig:auroc_random_label_function_performance}

We observed that including one label function of a mismatched type to distant supervision often improved performance, so we evaluated the effects of adding a random label function in the same setting.
We found that usually adding random noise did not improve performance (Figure {@fig:auroc_random_label_function_performance} and Supplemental Figure {@fig:aupr_random_label_function_performance}).
For the CbG edge type we did observe slightly increased performance via AUPR (Supplemental Figure {@fig:aupr_random_label_function_performance}).
However, performance changes in general were smaller than those observed with mismatched label types.

### Discriminative Model Performance

![
The discriminator model usually improves at a faster rate than the generative model as more edge-specific label function are included.
The line plot headers represents the specific edge type the discriminator model is trying to predict.
The x-axis shows the number of randomly sampled label functions that are incorporated on top of the baseline model (point at 0).
The y axis shows the area under the receiver operating curve (AUROC).
Each datapoint represents the average of each 50 sample run and the error bars represent the 95% confidence interval of each run.
The baseline and “All” data points consist of sampling from the entire fixed set of label functions.
This makes the error bars appear flat.
](https://raw.githubusercontent.com/danich1/snorkeling/1941485a02c8aa9972c67d8f9d3ff96acb0f3b7b/figures/disc_model_experiment/disc_model_test_auroc.png){#fig:auroc_discriminative_model_performance}

The discriminator model is designed to augment performance over the generative model by incorporating textual features along with estimated training labels.
The discriminative model is a piecewise convolutional neural network trained over word embeddings (See Methods).
We found that the discriminative model generally out-performed the generative model as more edge-specific label functions are incorporated (Figure {@fig:auroc_discriminative_model_performance} and Supplemental Figure {@fig:aupr_discriminative_model_performance}).
The discriminator model's performance is often poorest when very few edge-specific label functions are added to the baseline model (seen in Disease associates Gene (DaG), Compound binds Gene (CbG) and Gene interacts Gene (GiG)). 
This suggests that generative models trained with more label functions produce outputs that are more suitable for training discriminative models.
An exception to this trend is Compound treats Disease (CtD) where the discriminator model out-performs the generative model at all levels of sampling.
We observed the opposite trend with the Compound-binds-Gene (CbG) edges: the discriminator model was always poorer or indistinguishable from the generative model.
Interestingly, the AUPR for CbG plateaus below the generative model and the decreases when all edge-specific label functions are used (Supplemental Figure {@fig:aupr_discriminative_model_performance}).
This suggests that the discriminator model might be predicting more false positives in this setting.
Incorporating more edge-specific label functions usually improves performance for the discriminator model over the generator model.

### Discriminative Model Calibration

![
Deep learning models are overconfident in their predictions and need to be calibrated after training.
These are calibration plots for the discrimintative model.
The green line represents the predictions before calibration and the blue line shows predictions after calibration. 
Data points that lie closer to diagonal line show better model calibration, while data points far from the diagonal show poor performance.
A perfectly calibrated model would align straight along the diagonal line. 
](https://raw.githubusercontent.com/danich1/snorkeling/86037d185a299a1f6dd4dd68605073849c72af6f/figures/model_calibration_experiment/model_calibration.png){#fig:discriminative_model_calibration}

Even deep learning models with good AUROC and AUPR statistics can be subject to poor calibration.
Typically, these models are overconfident in their predictions [@arxiv:1706.04599; @arxiv:1807.00263].
We attempted to use temperature scaling to fix the calibration of the best performing discriminative models (Figure {@fig:discriminative_model_calibration}).
Before calibration (green lines), our models were aligned with the ideal calibration only when predicting low probability scores (close to 0.25).
Applying the temperature scaling calibration algorithm (blue lines) did not substantially improve the calibration of the model in most cases.
The exception to this pattern is the Disease associates Gene (DaG) model where high confidence scores are shown to be better calibrated.
Overall, calbrating deep learning models is a nontrivial task that requires  more complex approaches to accomplish.

### Text mined edges can expand a database-derived knowledge graph

![
Text-mined edges recreate a substantial fraction of an existing knowledge graph and include new predictions.
This bar chart shows the number of edges we can successfully recall in green and shows the number of new edges that can be added in blue.  
The recall for the Hetionet v1 knowledge graph is shown as a percentage in parentheses.
For example, for the compound treats disease edge our method recalls 85% of existing edges as adds 6,088 new edges.
](https://raw.githubusercontent.com/danich1/snorkeling/6e929e486537c5d105e393e20984b96910c96024/figures/edge_prediction_experiment/edges_added.png){#fig:hetionet_reconstruction}

One of the goals in our work is to measure the extent to which learning multiple edge types could construct a biomedical knowledge graph.
Hetionet v1 is a knowledge graph that was constructed from biomedical databases.
We measured this framework's recall and quantified how many new edges could be added with high confidence.
Overall, we were able to recall more than half of preexisting edges for all edge types (Figure {@fig:hetionet_reconstruction}) and report our top ten scoring sentences for each edge type in Supplemental Table {@tbl:edge_prediction_tbl}.
Our best recall is with the Compound treats Disease (CtD) edge type, where we retain 85% of preexisting edges.
Plus, we can add over 6,000 new edges to that category.
In contrast, we could only recall close to 70% of existing edges for the other categories; however, we can add over 40,000 novel edges to each category.
This highlights the fact that Hetionet v1 is missing a compelling amount of biomedical information and this framework is a viable way to close the information gap.
